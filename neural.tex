%% Hello emacs, this is -*- latex -*-
\typeout{ ====================================================================}
\typeout{ This is file neural.tex, created at 27-Aug-2005 }
\typeout{ Maintained by Andre Anjos <Andre.dos.Anjos@cern.ch> }
\typeout{ ====================================================================}

\chapter{Sistema neural de deteção elétron/jato baseado em calorimetria}
\label{chap:neural}

Neste capítulo, define-se um sistema de extração de característica em anéis de
deposição energética, associado a um sistema de discriminação neural, para o
projeto de um classificador de partículas ótimo para o Segundo Nível de
Filtragem do experimento ATLAS. Este classificador supera significativamente
em desempenho o sistema clássico de deteção que é usado como base no
experimento ATLAS. O sistema proposto tem ainda as vantagens de ser modular,
rápido e de simples manutenção.

\section{Revisão da literatura}
\label{sec:review}

%\textit{Introduzir um histórico de das diversas técnicas até hoje, emprego de
%RN em HEP, Hera, D0, LEP. Faltando...}

%Perfil:

%\begin{itemize}
%\item Entre 10-15 páginas máximo;
%\item[OK] Bruce Demby escreveu um review completo na NIM;
%\item[OK] Chrss Kiesling implementou RN no Trigger do H1, autor do processo de
%relevância? 
%\item Verificar publicações no CERN, Fermilab e Desy (\eng{via} SPIRES?);
%\item Verificar publicações nos congressos (IEEE, ACAT, ISCAS, de 2000 para
%cá); 
%\item[OK] Redes Neurais e mapeamento topológico em calorimetria (Wisconsin);
%\item Por alto como se fazia no LEP;
%\item \textbf{Seixas}: coloque aqui outras idéias...
%\end{itemize}

Os primeiros sistemas de deteção baseados em redes neurais artificiais,
aplicados à Física de altas energias, surgiram em 1987, na reconstrução de
partículas carregadas e píons neutros \cite{denby-nim-1997}. Hoje em dia
\cite{denby-nim-2004}, cerca de 20 anos passados, tais sistemas são empregados
em praticamente todas as áreas neste campo, desde da concepção de detetores
\cite{wilk-nim-2006}, filtragem e aquisição de dados
\cite{denby-nim-2003, kohne-nim-1997, varela-cms-1998},
passando pela reconstrução \eng{offline} \cite{peterson-nim-1988} e análise
\cite{kiesling-nim-2004}. Nestes ambientes, o reconhecimento de padrões pode
ocorrer tanto num plano mais elementar, onde deseja-se identificar os
parâmetros de cada partícula que surge de uma reação sub-atômica, quanto num
plano mais avançado, onde combina-se as informações detetadas para se tentar
definir as características globais de um evento.

\subsection{Redes neurais para análise e reconstrução \eng{offline}}

\subsection{Reconstrução de agrupamentos em calorímetros}

\subsection{Redes neurais em sistemas de filtragem}

A taxa de falsos-positivos em um experimento em física de altas energias pode
ser bastante grande, chegando muitas vezes a ser ordens de magnitude mais
elevadas que o sinal que se deseja detetar. Somado a este fator, incluem-se as
restrições de tempo de processamento de um sistema de filtragem que deva
operar em tempo real, processando, milhões de eventos por segundo.

O primeiro sistema de filtragem baseado em redes neurais foi construído e
testado em 1992, no Fermilab, Estados-Unidos, para o experimento $D0$ no
Tevatron \cite{lindsey-nim-1992}. Este sistema pode determinar os parâmetros
da trajetória de múons levando em consideração valores de tensão digitalmente
amostrados de uma câmara dividida em três planos, cada um contendo várias
células de deteção. Cada célula é equipada com placas coletoras de tensão que
permitem a amostragem da distância relativa da partícula com relação aos
dipolos através do atraso do sinal de avalanche disparado pela interação do
múon com a célula de deteção.

Levando-se em consideração os valores de atraso em cada célula e que a
partícula atravessará todas as camadas do detetor, é possível calcular o ponto
de impacto aproximado e ângulo da trajetória da partícula. Para tal, uma
rede neural tipo MLP com 64 neurônios escondidos e 64 saídas, baseada em
\eng{hardware} especializado (chip Intel ETANN), é treinada levando-se em
consideração os sinais de interação de cerca de 10.000 múons com este
detetor. Estes eventos foram simulados através de processos de Monte-Carlo. O
treinamento em si ocorre em um PC que emula o sistema em \eng{hardware}. Os
pesos podem então ser descarregados no processador e utilizados
\eng{online}.

Os resultados encontrados por este sistema são surpreendentemente positivos em
comparação com métodos de reconstrução \eng{offline}. Considerando-se um
conjunto de testes de alguns milhares de traços, o sistema baseado no
processador neural obtém um erro menor que 5 mm no ponto de impacto na
primeira camada de 96\% (contra 98\% de um sistema de \eng{fitting}
\eng{offline}). Para o coeficiente angular, este sistema possui um erro menor
que $5,7^{o}$ para 93\% (contra 97\% \eng{offline}) para os padrões de
teste. O tempo total de processamento do sistema é de apenas 8~$\mu$s.

O experimento H1, que utiliza o acelerador HERA no laboratório Desy em
Hamburgo, Alemanha, estuda a distribuição do momento dos constituentes de
prótons e mede a força de acoplamento do glúon aos diferentes quarks. Devido à
interações tais como as que ocorrem entre o feixe e resíduos no tubo do
acelerador, a taxa de eventos que constituem ruído de fundo chega a uma ordem
de $10^5$ comparada à ocorrência de física interessante. Para filtrar a taxa
de eventos inicial de cerca 10 milhões por segundo para apenas 100, este
experimento utiliza um sistema de filtragem divido em 4 níveis. O primeiro
nível de filtragem reduz a taxa de eventos em cerca de 100 vezes. Desta forma,
o segundo nível de filtragem tem apenas cerca de 10~$\mu$s para tomar uma
decisão. Este nível deve reduzir a taxa de eventos aproximadamente 10
vezes. Uma rede neural foi utilizada para resolver este problema
\cite{kohne-nim-1997}. 

No primeiro nível de filtragem, 16 variáveis são calculadas tais como a
energia total, transversal e na parte centrais do calorímetro, dentre
outras. Este nível de filtragem ignora, no entanto, quaisquer relações entre
estas variáveis. Cortes mais sofisticados podem ser executados no segundo
nível adicionado-se à estas variáveis, outras quantidades que somente estão
disponíveis após o tempo de decisão do primeiro nível e alimentando-se uma
rede neural tipo MLP. 

As variáveis selecionadas (somas de energias, número de partículas com carga
detetadas, etc.) são utilisadas para determinar se o evento em questão advem
de uma interação elétron-próton (física de interessante), de uma interação do
feixe com uma molécula no tubo do acelerador ou qualquer outro fenômeno a ser
desprezado. O sistema é implementado em \eng{hardware}, utilizando o
processador CNAPS 1064, da empresa Adaptive Solutions, executando em um tempo
total de 20~$\mu$s. Cada canal de decaimento é estudado separadamente e uma
rede especialista utilizada para detetar sua ocorrência.

A eficiência do sistema é variável conforme o canal em análise e apresentada
neste trabalho levando-se em consideração a taxa de redução de eventos
representando falso-alarme com relação ao primeiro nível de filtragem. Esta
taxa chega a valores entre 40 e 160 para eventos tipo $\gamma p \rightarrow X
+ J/\Psi$ e de 10 a 20 para eventos tipo $\gamma p \rightarrow \text{jatos}$,
o que está acima das especificações originais para o sistema de
filtragem. Nestes casos, o sistema neural permitiu que se elimine
complementamente pré-escalonamento no primeiro nível, aumentando
substancialmente o número de eventos interessantes registrados em mídia
permanente.

\subsection{Redes neurais e calorimetria em sistemas de filtragem}

Em \cite{varela-cms-1998}, descreve-se um sistema de deteção neural de
elétrons em tempo real que utiliza-se das informações dos calorímetros e.m. do
experimento CMS \cite{cms-trigger}. O primeiro nível de filtragem desta
experiência não exaure a informação provida pelo calorímetro e.m. e portanto
há oportunidade para que um segundo nível de filtragem, apoiando-se na
informação não aproveitada pelo LVL1, consiga atingir alguma redução na taxa
de eventos que será repassado ao terceiro nível de filtragem.

A informação utilizada para a deteção de elétrons são os valores de energia de
um conjunto de células provenientes dos cristais do calorímetro e.m.. Este
conjunto define um ``quadrado'' de aproximadamente 7 por 7 células deste
detetor, cobrindo uma área de aproximadamente $0,1 \times 0,1$ no plano $\eta
\times \phi$. O algoritmo escolhido inicia o processamento localizando o pico
de deposição energética na região. Em seguida, de forma a tornar o processo de
deteção independente da energia, normaliza-se o valor de deposição energética
em cada célula pelo valor de energia na célula mais energética. Para
amplificar os valores de deposição energética nas bordas da região em análise,
tira-se a raíz quadrada do valor normalizado das células.

Cada região analisada, composta de 49 valores de energia normalizada, é
disponibilizada como entrada a uma rede neural tipo MLP, com 8 neurônios
escondidos, treinada para detetar jatos e elétrons. A base de dados inicial
consistia de 50.000 jatos e 15.000 elétrons simulados com energia entre 5 e
225 GeV. Estes eventos são submetidos a uma filtragem por um sistema de
simulação do LVL1 do CMS, definindo-se a base de dados disponível para
treinamento: 1.500 jatos e por volta de 13.000 elétrons. Para uma taxa
constante de deteção de elétrons de 98\%, obteve-se uma taxa de deteção de
jatos de 71\% para eventos com ruído de fundo equivalente a um ambiente
simulado de alta luminosidade ($10^{34}\text{cm}^{-2}\text{s}^{-1}$) e 78\%,
para um ambiente em baixa luminosidade ($10^{33}\text{cm}^{-2}\text{s}^{-1}$).

\subsection{Tendências}

%Palavras-chave: pré-processamento, baixa latência, encapsulamento de ruído,
%determinação de parâmetros, MLP

%Describe stuff for ATLAS, like \cite{daq11}, \cite{daqnote00-02} and
%\cite{abramowicz-nim-2004}. 

\section{Discriminação linear}
\label{sec:lms}

O discriminador de Fisher \cite{fisher} ou a Análise de Discriminação Linear
descrevem algoritmos bem definidos para que se maximize a capacidade
discriminante de um corte no plano com N dimensões, que separa duas classes de
amostras \cite{duda}. Esta técnica requer que determinadas características nas
amostras estejam presentes, entre elas, que os dados apresentem uma
distribuição gaussiana e que as matrizes de covariância para ambas as classes
em separado sejam idênticas\footnote{A Análise de Discriminação Quadrática, no
entanto, demonstra que esta característica pode ser relaxada considerando-se
que seja sempre possível projetar, através de uma transformação linear, o
espaço de entrada em um outro espaço onde as matrizes de covariância sejam
iguais e portanto recaindo no caso simples.}. Muitas vezes, na prática, os
valores de média e a covariância não estão disponíveis e devem ser estimados,
o que normalmente leva a falhas no cálculo do ponto ótimo de
discriminação. Dentre as técnicas de estimação da média e covariância, pode-se
destacar a estimação por máxima verossimilhança ou por máximo a
\textit{posteriori} \cite{duda}.

De posse dos valores de média e covariância das classes, o plano de separação
seria definido da seguinte forma:

\begin{equation}
\overrightarrow{w} = \Sigma^{-1}(\overrightarrow{\mu}_0 -
\overrightarrow{\mu}_1) 
\label{eq:weight-1}
\end{equation}

O operador $\Sigma$ na Equação~(\ref{eq:weight-1}) representa a covariância ou
\textit{variância cruzada} das observações do universo de entrada e $\mu$ as
médias das duas classes de eventos que se deseja separar. Um problema que se
segue é da inversibilidade de $\Sigma$, que dependerá de quão bem o conjunto
de amostras representa as classes a serem discriminadas. Por exemplo, se
$\Sigma$ possui combinações lineares dos dados disponíveis, o ranque desta
matriz será inferior ao número de amostras e portanto a matriz não será
inversível. Embora existam maneiras de superar o problema, é possível fazer
uso de outras técnicas derivadas deste sistema primário para que se maximize a
discriminação das classes de eventos dado um conjunto de amostras.

Por causa das dificuldades discutidas, muitas técnicas iterativas surgiram
para que seja possível a definição de um plano ótimo de separação linear à
partir de amostras de dados reais. Dentre elas, o algoritmo do Mínimo Médio
Quadrático \cite{widrow} (do inglês \eng{Least Mean Square} ou LMS) é um dos
mais utilizados. Este método pode ser considerado um caso especial de uma rede
neural totalmente conectada e sem realimentação (veja Seção~\ref{sec:neural}
para uma discussão mais detalhada), com as seguintes ressalvas:

\begin{itemize}
\item Há somente um neurônio conectando todas as entradas com a saída da rede;
\item A função de ativação deste neurônio ($\phi(\dot)$) é a função identidade
$f(x) = x$.
\end{itemize}

Para o treinamento, definem-se alvos para as duas classes de eventos e a
função de erro:

\begin{equation}
E(\overrightarrow{w}) = \frac{1}{2} e^2(n)
\label{eq:mse-definition}
\end{equation}

Nesta equação, $\overrightarrow{w}$ é o vetor de pesos que define o plano de
separação linear e $e(n)$ é o erro na saída da rede com relação ao alvo para a
classe escolhida na amostra $n$, ou seja $e(n) = d(n)-y(n)$ ($d(n)$ é o
alvo). Desta forma, derivando-se esta função de erro com relação aos pesos
sinápticos $\overrightarrow{w}$, mostra-se que o gradiente de $E$ será:

\begin{equation}
\frac{\partial E(\overrightarrow{w})}{\partial \overrightarrow{w}} =
-x(n)e(n) 
\end{equation}

Nesta equação, $x(n)$ representa a n-ésima entrada que leva o neurônio linear
a uma saída $y(n)$. E, com este resultado, define-se a fórmula de treinamento
do LMS:

\begin{equation}
\hat{w}(n+1) = \hat{w}(n) + \alpha x(n)e(n)
\label{eq:lms}
\end{equation}

Nesta equação, $\alpha$ representa a taxa de aprendizagem, um parâmetro para a
suavização da trajetória do treinamento no espaço da função de erro
$E(\overrightarrow{w})$. A Equação~(\ref{eq:lms}) representa a fórmula
clássica do treinamento de um classificador LMS, indicando um método
bem-definido de atualização dos pesos da rede para que se convirja a um erro
mínimo. Em implementações realísticas de um classificador LMS, é possível
agrupar a correção dos pesos $\hat{w}$ em bateladas ou épocas de forma que se
suavize a migração do sistema para o mínimo global. No treinamento em épocas,
os pesos sinápticos são corrigidos tendo por base a média dos erros para todos
os eventos de uma época, ao invés da atualização instantânea proposta pela
Equação~(\ref{eq:lms}).

Como é possível inspecionar diretamente na Equação~(\ref{eq:lms}), os pesos do
detetor são atualizados segundo um produto interno dos erros multiplicados
pelas respectivas entradas, uma época do treinamento. No caso de existirem
grandes diferenças de magnitude entre as variáveis de entrada de um sistema
LMS, é possível que o processo de treinamento se torne tendencioso em função
destas variáveis. Para eliminar o risco de tendência no treinamento devido a
magnitude das componentes da entrada, é também prática que cada entrada seja
subtraída da sua média e dividida pela sua variância.

Finalmente, a Figura~\ref{fig:lms-flow} mostra o diagrama de blocos do
discriminador LMS que será empregado neste estudo. O valor da saída $y$ é
utilizado para definir a R.O.C. do discriminador e escolher, ao invés de
quatro, apenas um corte que maximize a capacidade discriminante do
sistema. Alvos ($t$) para cada uma das classes são escolhidos e o sinal de
erro é utilizado para corrigir os pesos sinápticos, até que o sistema convirja
para o erro mínimo.

\begin{figure}
\begin{center}
\includegraphics{lms-flow}
\end{center}
\caption{O diagrama de fluxo do discriminador LMS que será empregado na
discriminação elétron-jato.}
\label{fig:lms-flow}
\end{figure}

\subsection{Implementação do LMS na separação e\-lé\-tron-jato}
\label{sec:lms-ej}

Levando-se em consideração as 4 variáveis definidas pelo T2Calo, é possível
encontrar o plano quadri-dimensional que define um discriminador linear ótimo,
usando-se o algoritmo LMS, como descrito anteriormente. A separação em classes
de treinamento e teste, como apresentada na Seção~\ref{sec:eghypo}, será
re-aproveitada aqui, para que seja possível a comparação dos resultados nos
dois casos.

O sistema de treinamento e teste foi implementado em um ambiente de trabalho
C++, seguindo o paradigma da orientação à objetos (OO) \cite{stroustrup,
booch}. Este ambiente, que será re-utilizado em várias partes deste trabalho,
será discutido em detalhes na Seção~\ref{sec:framework}. A
Figura~\ref{fig:train-flow} contém um diagrama de fluxo com os diversos passos
do sistema de treinamento implementando. Inicialmente, os bancos de dados
usados para o treinamento e teste da rede são carregados. Os valores de média
e variância dos dados, levando-se em consideração a estatística de cada uma
das classes de dados, são extraídos do conjunto de treinamento e guardados
juntos aos pesos $\hat{w}(n)$, para que sejam aplicados durante o
treinamento\footnote{De fato, seria mais eficiente que os fatores de
normalização fossem aplicados previamente ao treinamento. No entanto, uma vez
que deseja-se re-aproveitar a base de código para rodar o discriminador em uma
etapa seguinte, é mais simples do ponto de vista da implementação que a
normalização seja aplicada como parte do passo de execução do
discriminador.}. Os pesos sinápticos são aleatoriamente inicializados (entre
$-1$ e $+1$). Valores-alvo para cada uma das classes são pré-fixados: $-1$
para elétrons e $+1$ para jatos. O processo de treinamento é então
disparado. Para cada época ou batelada de treinamento, um conjunto de elétrons
e jatos é escolhido aleatoriamente à partir dos bancos-de-dados
disponíveis. Os valores de erro são calculados e sua média é aplicada para a
correção dos pesos sinápticos, considerando a taxa de aprendizagem
$\alpha$. As melhores redes, tomando-se o valor do produto SP como referência,
são guardadas a cada passo de treinamento. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{train-flow}
\end{center}
\caption{O fluxo implementado para o treinamento do discriminador LMS.}
\label{fig:train-flow}
\end{figure}

Um sistema configurável é utilizado para detetar a estagnação do
treinamento. Este sistema baseia-se na observação do valor do produto SP para
o conjunto de teste. Quando este valor atinge um estado no qual as alterações
sinápticas não o modificam significativamente (levando-se em consideração uma
margem de erro configurável), e por um número de iterações, o treinamento é
automaticamente parado. O melhor discriminador até então é recarregado. Um
estudo dos valores de Erro Médio Quadrático (EMQ, do inglês \eng{Mean-square
error}, MSE) é feito para os conjuntos de treinamento e teste. O produto SP
máximo também é avaliado para os dois conjuntos de dados. Esta técnica tem o
objetivo de evitar um treinamento excessivo da rede (do inglês
\eng{overtraining}) fazendo com que ela se especialize em demasiado nestes
dados sem conseguir ter uma capacidade de discriminação generalizada. O valor
da variação do produto SP a partir do qual considera-se que o sistema esteja
estagnado dependerá do problema. Valores típicos estão na ordem de $10^{-3}$.

Para este trabalho, optou-se pela a não utilização de um conjunto de validação
do sistema de deteção. Este conjunto poderia ser utilizado para verificar a
operação do sistema após o treinamento. No entanto, a física disponível para o
estudo é limitada devido a natureza de geração de eventos. Para se realizar
uma análise baseada no LVL2, elétrons e jatos que passem a um corte do LVL1
devem ser simulados, uma vez que o LHC ainda não encontra-se operacional. 

A taxa de redução do LVL1 é bastante expressiva, atingindo uma redução de
aproximadamente 1000 vezes na taxa de contaminação de elétrons. Desta forma,
para produzir 1000 jatos que passariam a um corte do LVL1 é necessário que se
produza cerca de 1 milhão de eventos deste tipo. Cada simulação de um evento
pode durar muitos minutos, às vezes horas de processamento do poder de
computação disponível.

%Por outro lado, a física simulada é representativa do tipo de partículas
%que deseja-se analisar.

\subsection{Resultados da aplicação do LMS na separação e\-lé\-tron-jato}

Utilizando o sistema descrito anteriormente e a separação dos dados (em
classes de treinamento e teste) proposta no início do estudo, realizou-se um
conjunto de testes para a determinação de um discriminador linear ótimo
baseado nas quatro variáveis extraídas pelo T2Calo. Dois parâmetros foram
considerados neste exercício:

\begin{itemize}
\item O número de elementos na batelada ($N$);
\item O valor da taxa de treinamento ($\alpha$).
\end{itemize}

Embora seja possível assumir que o sistema convergirá em todas as condições,
valores muito grandes para a taxa de treinamento (ou pequenos para a
quantidade de eventos na batelada) poderão fazer o processo de convergência ao
mínimo parecer instável e disparar erros no sistema de parada automática do
treinamento. Desta forma, conduziu-se um conjunto de testes para a
determinação dos parâmetros de treinamento que acarretem em uma migração suave
ao erro mínimo do discriminador, no menor tempo possível. Os resultados podem
ser vistos nas Figuras~\ref{fig:lms-lr-analysis} e
\ref{fig:lms-esize-analysis}. Cada ponto nos gráficos representa a média sobre
dez treinamentos realizados à partir de um conjunto de pesos iniciais
sorteados (uniformemente) aleatoriamente no intervalo $[-1, +1]$. O desvio
padrão com relação a média define o tamanho da barra de erro. Durante o
experimento, limitou-se o número máximo de passos de treinamento em
$1.000$. Desta forma, pontos nas curvas próximos a este valor no eixo vertical
indicam provável que o treinamento foi interrompido bruscamente pelo programa.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/lms-lr-analysis}
\end{center}
\caption{Número de passos de treinamento médio para um detetor LMS baseado nas
4 características extraídas pelo T2Calo, em função da taxa de treinamento.}
\label{fig:lms-lr-analysis}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/lms-esize-analysis}
\end{center}
\caption{Número de passos de treinamento médio para um detetor LMS baseado nas
4 características extraídas pelo T2Calo, em função do número de eventos por
época de treinamento.}
\label{fig:lms-esize-analysis}
\end{figure}

%\begin{table}
%\begin{center}
%\renewcommand{\baselinestretch}{1.0}
%{\tiny
%\begin{tabular}{|r|r|r|r|r|r|} \hline
%Taxa de treinamento ($\alpha$) & Época & Número de passos & Máximo SP (treino)
%& Máximo SP (teste) \\ \hline \hline 
%$0.0010$ & $10$ & $267\pm117.3$ & $1.5024\pm1.4\times 10^{-2}$ & $1.4975\pm1.7\times 10^{-2}$ \\ \hline
%$0.0010$ & $50$ & $235\pm50.8$ & $1.5045\pm2.1\times 10^{-2}$ & $1.5021\pm2.2\times 10^{-2}$ \\ \hline
%$0.0010$ & $100$ & $238\pm74.3$ & $1.5159\pm1.9\times 10^{-2}$ & $1.5114\pm1.6\times 10^{-2}$ \\ \hline
%$0.0010$ & $200$ & $204\pm62.0$ & $1.5193\pm1.6\times 10^{-2}$ & $1.5159\pm1.7\times 10^{-2}$ \\ \hline
%$0.0010$ & $500$ & $206\pm60.1$ & $1.5126\pm1.6\times 10^{-2}$ & $1.5078\pm1.7\times 10^{-2}$ \\ \hline
%$0.0010$ & $1000$ & $214\pm61.2$ & $1.5073\pm1.8\times 10^{-2}$ & $1.4996\pm1.7\times 10^{-2}$ \\ \hline
%$0.0010$ & $2000$ & $236\pm52.3$ & $1.5054\pm1.6\times 10^{-2}$ & $1.5009\pm1.4\times 10^{-2}$ \\ \hline
%$0.0020$ & $10$ & $216\pm42.7$ & $1.5122\pm1.0\times 10^{-2}$ & $1.5068\pm1.1\times 10^{-2}$ \\ \hline
%$0.0020$ & $50$ & $179\pm32.3$ & $1.5159\pm1.3\times 10^{-2}$ & $1.5129\pm1.1\times 10^{-2}$ \\ \hline
%$0.0020$ & $100$ & $151\pm29.4$ & $1.5111\pm1.4\times 10^{-2}$ & $1.5095\pm1.4\times 10^{-2}$ \\ \hline
%$0.0020$ & $200$ & $182\pm40.4$ & $1.5142\pm1.4\times 10^{-2}$ & $1.5088\pm1.6\times 10^{-2}$ \\ \hline
%$0.0020$ & $500$ & $160\pm42.0$ & $1.5077\pm2.0\times 10^{-2}$ & $1.5060\pm2.1\times 10^{-2}$ \\ \hline
%$0.0020$ & $1000$ & $165\pm34.2$ & $1.5089\pm1.7\times 10^{-2}$ & $1.5058\pm1.4\times 10^{-2}$ \\ \hline
%$0.0020$ & $2000$ & $150\pm35.8$ & $1.5080\pm1.3\times 10^{-2}$ & $1.5037\pm1.1\times 10^{-2}$ \\ \hline
%$0.0050$ & $10$ & $385\pm267.5$ & $1.5096\pm8.6\times 10^{-3}$ & $1.5053\pm7.6\times 10^{-3}$ \\ \hline
%$0.0050$ & $50$ & $108\pm33.8$ & $1.5150\pm1.2\times 10^{-2}$ & $1.5113\pm9.0\times 10^{-3}$ \\ \hline
%$0.0050$ & $100$ & $124\pm35.9$ & $1.5158\pm1.7\times 10^{-2}$ & $1.5096\pm1.5\times 10^{-2}$ \\ \hline
%$0.0050$ & $200$ & $105\pm23.2$ & $1.5191\pm1.4\times 10^{-2}$ & $1.5154\pm1.4\times 10^{-2}$ \\ \hline
%$0.0050$ & $500$ & $104\pm21.8$ & $1.5098\pm5.4\times 10^{-3}$ & $1.5066\pm6.3\times 10^{-3}$ \\ \hline
%$0.0050$ & $1000$ & $119\pm66.2$ & $1.5125\pm8.8\times 10^{-3}$ & $1.5100\pm8.0\times 10^{-3}$ \\ \hline
%$0.0050$ & $2000$ & $96\pm30.1$ & $1.5148\pm1.2\times 10^{-2}$ & $1.5123\pm1.2\times 10^{-2}$ \\ \hline
%$0.0100$ & $10$ & $736\pm422.9$ & $1.5153\pm6.9\times 10^{-3}$ & $1.5103\pm6.9\times 10^{-3}$ \\ \hline
%$0.0100$ & $50$ & $413\pm409.9$ & $1.5183\pm1.3\times 10^{-2}$ & $1.5140\pm1.1\times 10^{-2}$ \\ \hline
%$0.0100$ & $100$ & $284\pm228.8$ & $1.5116\pm1.0\times 10^{-2}$ & $1.5067\pm5.8\times 10^{-3}$ \\ \hline
%$0.0100$ & $200$ & $136\pm73.4$ & $1.5156\pm1.5\times 10^{-2}$ & $1.5124\pm1.5\times 10^{-2}$ \\ \hline
%$0.0100$ & $500$ & $107\pm36.0$ & $1.5145\pm1.2\times 10^{-2}$ & $1.5086\pm1.1\times 10^{-2}$ \\ \hline
%$0.0100$ & $1000$ & $121\pm46.1$ & $1.5122\pm1.5\times 10^{-2}$ & $1.5083\pm1.2\times 10^{-2}$ \\ \hline
%$0.0100$ & $2000$ & $94\pm35.7$ & $1.5136\pm1.1\times 10^{-2}$ & $1.5086\pm8.9\times 10^{-3}$ \\ \hline
%$0.0200$ & $10$ & $999\pm0.0$ & $1.5189\pm1.1\times 10^{-2}$ & $1.5130\pm1.1\times 10^{-2}$ \\ \hline
%$0.0200$ & $50$ & $993\pm17.4$ & $1.5152\pm1.4\times 10^{-2}$ & $1.5108\pm1.4\times 10^{-2}$ \\ \hline
%$0.0200$ & $100$ & $908\pm286.5$ & $1.5186\pm1.2\times 10^{-2}$ & $1.5156\pm1.2\times 10^{-2}$ \\ \hline
%$0.0200$ & $200$ & $736\pm423.5$ & $1.5195\pm1.4\times 10^{-2}$ & $1.5132\pm1.3\times 10^{-2}$ \\ \hline
%$0.0200$ & $500$ & $338\pm325.4$ & $1.5126\pm1.2\times 10^{-2}$ & $1.5097\pm1.2\times 10^{-2}$ \\ \hline
%$0.0200$ & $1000$ & $135\pm129.9$ & $1.5173\pm1.1\times 10^{-2}$ & $1.5113\pm1.0\times 10^{-2}$ \\ \hline
%$0.0200$ & $2000$ & $131\pm75.2$ & $1.5173\pm1.1\times 10^{-2}$ & $1.5128\pm1.2\times 10^{-2}$ \\ \hline
%$0.0500$ & $10$ & $999\pm0.0$ & $1.5263\pm1.4\times 10^{-2}$ & $1.5188\pm1.3\times 10^{-2}$ \\ \hline
%$0.0500$ & $50$ & $999\pm0.0$ & $1.5134\pm8.2\times 10^{-3}$ & $1.5117\pm9.3\times 10^{-3}$ \\ \hline
%$0.0500$ & $100$ & $999\pm0.0$ & $1.5239\pm1.2\times 10^{-2}$ & $1.5206\pm1.2\times 10^{-2}$ \\ \hline
%$0.0500$ & $200$ & $999\pm0.0$ & $1.5170\pm1.1\times 10^{-2}$ & $1.5113\pm1.1\times 10^{-2}$ \\ \hline
%$0.0500$ & $500$ & $999\pm0.0$ & $1.5109\pm9.6\times 10^{-3}$ & $1.5057\pm7.8\times 10^{-3}$ \\ \hline
%$0.0500$ & $1000$ & $999\pm0.0$ & $1.5154\pm1.1\times 10^{-2}$ & $1.5119\pm9.5\times 10^{-3}$ \\ \hline
%$0.0500$ & $2000$ & $906\pm220.9$ & $1.5122\pm1.2\times 10^{-2}$ & $1.5087\pm1.4\times 10^{-2}$ \\ \hline
%$0.1000$ & $10$ & $999\pm0.0$ & $1.5332\pm5.5\times 10^{-3}$ & $1.5246\pm8.4\times 10^{-3}$ \\ \hline
%$0.1000$ & $50$ & $999\pm0.0$ & $1.5256\pm1.4\times 10^{-2}$ & $1.5177\pm1.2\times 10^{-2}$ \\ \hline
%$0.1000$ & $100$ & $999\pm0.0$ & $1.5229\pm7.5\times 10^{-3}$ & $1.5143\pm4.9\times 10^{-3}$ \\ \hline
%$0.1000$ & $200$ & $999\pm0.0$ & $1.5174\pm1.3\times 10^{-2}$ & $1.5101\pm1.1\times 10^{-2}$ \\ \hline
%$0.1000$ & $500$ & $999\pm0.0$ & $1.5072\pm1.0\times 10^{-2}$ & $1.5023\pm7.2\times 10^{-3}$ \\ \hline
%$0.1000$ & $1000$ & $999\pm0.0$ & $1.5188\pm1.2\times 10^{-2}$ & $1.5149\pm1.2\times 10^{-2}$ \\ \hline
%$0.1000$ & $2000$ & $999\pm0.0$ & $1.5209\pm8.9\times 10^{-3}$ & $1.5167\pm6.6\times 10^{-3}$ \\ \hline
%$0.1500$ & $10$ & $999\pm0.0$ & $1.5385\pm8.0\times 10^{-3}$ & $1.5271\pm4.4\times 10^{-3}$ \\ \hline
%$0.1500$ & $50$ & $999\pm0.0$ & $1.5251\pm9.5\times 10^{-3}$ & $1.5178\pm1.0\times 10^{-2}$ \\ \hline
%$0.1500$ & $100$ & $999\pm0.0$ & $1.5203\pm1.3\times 10^{-2}$ & $1.5154\pm1.1\times 10^{-2}$ \\ \hline
%$0.1500$ & $200$ & $999\pm0.0$ & $1.5178\pm1.1\times 10^{-2}$ & $1.5112\pm9.7\times 10^{-3}$ \\ \hline
%$0.1500$ & $500$ & $999\pm0.0$ & $1.5180\pm1.1\times 10^{-2}$ & $1.5125\pm9.6\times 10^{-3}$ \\ \hline
%$0.1500$ & $1000$ & $999\pm0.0$ & $1.5150\pm8.5\times 10^{-3}$ & $1.5086\pm8.6\times 10^{-3}$ \\ \hline
%$0.1500$ & $2000$ & $999\pm0.0$ & $1.5138\pm1.1\times 10^{-2}$ & $1.5076\pm1.0\times 10^{-2}$ \\ \hline
%$0.2000$ & $10$ & $999\pm0.0$ & $1.5409\pm5.5\times 10^{-3}$ & $1.5329\pm4.8\times 10^{-3}$ \\ \hline
%$0.2000$ & $50$ & $999\pm0.0$ & $1.5242\pm9.9\times 10^{-3}$ & $1.5182\pm6.7\times 10^{-3}$ \\ \hline
%$0.2000$ & $100$ & $999\pm0.0$ & $1.5256\pm9.1\times 10^{-3}$ & $1.5149\pm7.1\times 10^{-3}$ \\ \hline
%$0.2000$ & $200$ & $999\pm0.0$ & $1.5214\pm9.3\times 10^{-3}$ & $1.5143\pm7.5\times 10^{-3}$ \\ \hline
%$0.2000$ & $500$ & $999\pm0.0$ & $1.5201\pm8.3\times 10^{-3}$ & $1.5140\pm6.7\times 10^{-3}$ \\ \hline
%$0.2000$ & $1000$ & $999\pm0.0$ & $1.5248\pm1.3\times 10^{-2}$ & $1.5174\pm1.2\times 10^{-2}$ \\ \hline
%$0.2000$ & $2000$ & $999\pm0.0$ & $1.5133\pm1.1\times 10^{-2}$ & $1.5080\pm1.1\times 10^{-2}$ \\ \hline
%\end{tabular}
%}
%\renewcommand{\baselinestretch}{1.5}
%\end{center}
%\caption{Média de passos de treinamento e máximos de produto SP em função da
%taxa de aprendizagem e tamanho da época para um classificador LMS elétron/jato
%baseado nas saídas do T2Calo.}
%\label{tab:lms-scan}
%\end{table}

Uma análise visual nestes gráficos indica que valores de taxa de treinamento
que minimizem o tempo de treinamento estão na faixa onde $\alpha < 0.02$,
enquanto que para o tamanho da época, em $N \geq 500$. Escolhe-se $N = 500$ e
$\alpha = 0.01$. Desta forma, depois de cerca de 10 passos, $5000$ eventos
terão sido apresentados a rede. Com 40 ou 50 passos, o sistema de treinamento
terá, possivelmente, apresentado todos os dados de treinamento ao menos uma
vez.\footnote{O sorteio dos eventos de uma batelada é feito de forma aleatória
seguindo uma distribuição uniforme e não há garantias que todos os eventos
sejam utilizados ao longo do treinamento, embora seja estatisticamente
provável que isto aconteça após um certo número de épocas. Esta probabilidade
aumenta com o número de eventos por época e com o número de épocas de
treinamento a qual a rede foi submetida.}.

A Figura~\ref{fig:lms-mse-evo} mostra a evolução do EMQ para uma das 10 redes
com $N = 500$ e $\eta = 0.01$, tanto para o conjunto de treinamento (no alto)
quanto para o conjunto de teste (embaixo). A Figura~\ref{fig:lms-sp-evo}
mostra um gráfico equivalente, mas para o produto SP, ao invés do EMQ. Como é
possível verificar, o sistema converge depois de pouco menos que 100 passos de
treinamento. A evolução do produto SP aumenta rapidamente no início do
treinamento, por cerca de 20 passos e depois permanecendo constante, em
aproximadamente 1,5 para o restante do treinamento. O mesmo não ocorre para o
EMQ que ainda continuará a diminuir, significativamente, por mais 60 a 80
passos antes da deteção automática da parada. Este comportamento indica que,
apesar do sistema conseguir aproximar melhor os alvos na saída, sua capacidade
discriminatória mantém-se praticamente inalterada após os 20 passos
iniciais. O perfil de evolução do produto SP e minimização do EMQ é seguido
pelo conjunto de teste, de forma bastante semelhante, indicando que a
estatística disponível no conjunto treino é representativa dos dados no
conjunto de teste.

%É interessante notar que após 20 passos de treinamento, $10000$
%elementos terão sido utilizados no treinamento da rede. Este valor é menor que
%o tamanho do conjunto de treinamento, com cerca de $15000$ elementos,
%indicando que o conjunto de dados, para a análise linear, ...

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/mse-evolution}
\end{center}
\caption{Evolução dos valores do E.M.Q. ao longo do treinamento do detetor
LMS, para o conjunto de treinamento (topo) e de teste (embaixo). A taxa de
treinamento para este sistema é $0,01$ equanto que o tamanho da época
escolhido é $500$.}
\label{fig:lms-mse-evo}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/sp-evolution}
\end{center}
\caption{Evolução dos valores do produto SP ao longo do treinamento do detetor
LMS, para o conjunto de treinamento (topo) e de teste (embaixo). A taxa de
treinamento para este sistema é $0,01$ equanto que o tamanho da época
escolhido é $500$.}
\label{fig:lms-sp-evo}
\end{figure}

A Figura~\ref{fig:lms-test-output} mostra os histogramas da saída da rede para
elétrons (alto) e jatos (embaixo), considerando-se o conjunto de teste. O
ponto ótimo de corte, determinado para que se maximize o produto SP da rede
para o conjunto de treinamento para este sistema é $-0,0473$. Nestas
condições, o máximo produto SP para o conjunto de teste é 1,51, definido no
ponto da curva R.O.C. (veja Figura~\ref{fig:lms-test-roc}) onde a eficiência
para a deteção de elétrons é 91,64\% enquanto que a eficiência para a deteção
de jatos é de 90,35\%. A Figura~\ref{fig:lms-vs-egamma-roc} mostra uma
comparativo entre o resultado da otimização proposta atualmente no experimento
ATLAS contra os resultados obtidos para este classificador. Através desta
figura é possível observar que o discriminador LMS tangencia a parte exterior
dos pontos da otimização proposta atualmente no experimento. O resultado
obtido com o algoritmo LMS, ao invés do longo processo de otimização proposto
originalmente, é atingido depois de apenas 1 minuto e 30 segundos de
treinamento em um computador dotado de um processador AMD 64 com \eng{clock}
de 2~GHz e 1~Gb de memória RAM. Ademais, tal sistema apresenta uma capacidade
discriminatória praticamente ótima após apenas 20 passos de treinamentos, isto
é, $\approx 25$ segundos na máquina citada.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/test-output}
\end{center}
\caption{Saída do detetor LMS em estudo para o conjunto de treino.}
\label{fig:lms-test-output}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/test-roc}
\end{center}
\caption{Curva R.O.C. para um detetor elétron/jato baseado no detetor LMS em
estudo.}
\label{fig:lms-test-roc}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/lms-vs-egamma-roc}
\end{center}
\caption{R.O.Cs comparativas entre a otimização atual para o EGammaHypo e um
detetor baseado no LMS.}
\label{fig:lms-vs-egamma-roc}
\end{figure}

As Figuras~\ref{fig:lms-test-sp-eta}, \ref{fig:lms-test-sp-phi} e
\ref{fig:lms-test-sp-emet} mostram, respectivamente, o valor do produto SP segundo
a distribuição dos dados em $\eta$, $\phi$ e $\etem$. O número anexo ao topo
de cada barra indica a quantidade de cada canal e é útil para que se considere
a relevância de um valor parcial no desempenho do discriminador. Nota-se que
na distribuição por $\eta$, há uma clara perda de eficiência na região
$|\eta| \approx 1,5$. Esta notável queda no desempenho do classificador é
devido a um espaço sem elementos de deteção nesta área (também conhecido como
\eng{gap} ou \eng{crack} dos calorímetros), por onde passam os cabos de
leitura e manutenção dos detetores internos. A eficiência é recuperada logo
após esta região. O gráfico de barras para a distribuição $\phi$ mostra-se
praticamente uniforme, indicando que o sistema funciona sem tendências para
todos os valores desta variável. Este resultado é esperado, já que o detetor é
simétrico neste eixo. A análise por $\etem$ demonstra uma predominante
qualidade de deteção para valores mais baixos de energia que valores mais
altos. Deve-se levar em conta que, embora a eficiência de deteção de elétrons
ainda seja máxima nesta última região, como mostra a
Figura~\ref{fig:lms-test-efficiency-emet}, o falso alarme na deteção de jatos
também apresenta-se máximo. Uma vez que o produto SP é uma figura de mérito da
eficiência agregada de ambas as classes de eventos do discriminador, ela
quantificar-se-á em zero nesta região. Este resultado está associado a baixa
estatística de dados disponível na região.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/test-sp-eta}
\end{center}
\caption{Análise do produto SP do detetor LMS, para os dados do conjunto de
teste ao longo de $\eta$.}
\label{fig:lms-test-sp-eta}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/test-sp-phi}
\end{center}
\caption{Análise do produto SP do detetor LMS, para os dados do conjunto de
teste ao longo de $\phi$.} 
\label{fig:lms-test-sp-phi}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/test-sp-emet}
\end{center}
\caption{Análise do produto SP do detetor LMS, para os dados do conjunto de
teste por $\etem$.} 
\label{fig:lms-test-sp-emet}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/test-efficiency-emet}
\end{center}
\caption{Análise da eficiência de deteção de elétrons e falso alarme em jatos
para o detetor LMS, utilizando os dados do conjunto de teste, por $\etem$.}
\label{fig:lms-test-efficiency-emet}
\end{figure}

A Figura~\ref{fig:lms-vs-egamma} mostra um comparativo entre as duas técnicas
de deteção, por energia transversa na seção e.m.. Distingue-se que os dois
sistemas possuem respostas bastante próximas. Na primeira (mais à esquerda)
faixa de energia considerada, o EGammaHypo possui uma resposta melhor enquanto
que com o aumento da energia transversa o LMS apresenta-se mais eficiente, e
segue este padrão para quase todas as faixas consideradas, principalmente onde
há maior concentração de eventos de teste, como é possível determinar à partir
da Figura~\ref{fig:lms-test-sp-emet}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/lms-vs-egamma}
\end{center}
\caption{Comparação do produto SP para um classificador baseado no LMS e o
EGammaHypo por energia transversa na seção e.m..}
\label{fig:lms-vs-egamma}
\end{figure}

\subsection{Relevância das características do T2Calo}

A técnica da análise de relevância \cite{relevance} tem por objetivo medir a
importância de cada uma das variáveis de entrada para um classificador. Nesta
técnica, suprime-se a contribuição da variável à composição da saída
substituindo-se seu valor, a cada evento, por sua média, para todos os eventos
disponíveis na entrada do sistema de discriminação. Observando-se a variação
da saída, é possível estimar a contribuição daquela variável ao processo
discriminatório. A estimativa de relevância da i-ésima componente de entrada
pode, então ser calculada pela fórmula:

\begin{equation}
R_i = \frac{1}{N} \text{ } \overset{N}{\underset{j=1}{\sum}} \text{ }
[\text{saída}(\overrightarrow{x_j}) -
\text{saída}(\overrightarrow{x_j}\mid_{x_{j,i} = \overline{x}_i})]^2 
\label{eq:relevance-mse}
\end{equation}

Em outras palavras, é possível definir a relevância da variável $x_i$, $R_i$,
como o EMQ da saída de um discriminador comparada a mesma saída quando faz-se
a variável assumir o valor de sua média. Nessa equação, $N$ representa o
número total de eventos (padrões) disponíveis para o classificador. A
Figura~\ref{fig:relevance-mse} mostra os valores de relevância para cada uma
das variáveis do classificador em análise. Esta figura mostra, mais uma vez,
que o conjunto de treino permite generalizar o comportamento do discriminador,
apresentando valores de relevância, para cada uma de suas variáveis, bastante
próximos daqueles valores calculados para o conjunto de teste. Esta figura
também mostra que a variável $\rcore$ é de extrema importância na determinação
da saída do detetor. Quando substituímo-la por sua média, o erro na saída
aumenta de mais de 3 pontos enquanto que para as demais variáveis, o impacto é
nitidamente menor. Esta figura também indica que a ordem de separação proposta
pelo EGammaHypo está em acordo com o processo de filtragem definido
automaticamente pelo LMS, levando-se em consideração a ordem onde os cortes
são aplicados por aquele algoritmo e a importância das variáveis nesse
processo de classificação. A importância destas variáveis ao sistema LMS
poderia ser utilizada para simplificá-lo. Por exemplo, seria possível
``podar'' a variável menos relevante e re-treinar o sistema para obter um
classificador mais rápido.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/relevance-mse}
\end{center}
\caption{Os valores de relevância para os conjunto de treino (cinza claro) e
teste (em cinza escuro), para as 4 variáveis do T2Calo e considerando-se o
classificador LMS em estudo.}
\label{fig:relevance-mse}
\end{figure}

Por outro lado, como foi visto nas análises de evolução do EMQ
(Figura~\ref{fig:lms-mse-evo}) e do produto SP (Figura~\ref{fig:lms-sp-evo}),
a capacidade discriminatória da rede reage de forma correlacionada, mas
\textbf{não necessariamente} idêntica ao processo de mapeamento da entrada
na saída proposto pelo LMS. Observa-se que, apesar de notarmos uma contínua
migração ao mínimo EMQ durante cerca de 100 passos de treinamento, após cerca
de apenas 20 passos, a capacidade discriminatória do sistema já atinge seu
máximo e ali permanece até o final do treinamento. Este comportamento indica
que exista uma diferença não-negligível entre estes dois parâmetros. Desta
forma, propõe-se uma variação do cálculo da relevância, mas agora baseando-se
no impacto da substituição do valor da variável pela sua média à capacidade
discriminatória da rede, agora representada pela variação do produto SP, da
seguinte forma:

\begin{equation}
R_{d_i} = \max(\text{SP}_{\text{original}}) - \max(\text{SP}(\overrightarrow{x_j}\mid_{x_{j,i} = \overline{x}_i}))
\label{eq:relevance-sp}
\end{equation}

Neste caso define-se a relevância $R_{d_i}$ da variável $i$ como a diferença
no produto SP máximo causado pela substituição desta variável pela sua
média. De fato, espera-se que a maior parte dos valores de relevância sejam
positivos, indicando uma degradação do desempenho do classificador seguindo a
neutralização de uma variável. Seria possível que se encontrasee valores de
relevância negativos, o que indicaria que uma variável está, na verdade,
atrapalhando o processo de classificação ao invés de melhorá-lo. A
Figura~\ref{fig:relevance-sp} mostra os valores de $R_d$ para o classificador
LMS em estudo. Como é possível ver nesta figura, o quadro é marginalmente
diferente daquele mostrado pela Figura~\ref{fig:relevance-mse}. A variável
mais importante para o processo discriminatório é ainda $\rcore$, seguindo-se
de $\eratio$ e $\ethad$. A variável $\etem$ é a menos relevante para o
processo de classificação de elétrons e jatos, o que já era esperado
observando-se as tendências com o EGammaHypo. Como é possível definir à partir
deste gráfico, seria preferencial uma poda da variável $\etem$ que da variável
$\ethad$, como sugeriria a Figura~\ref{fig:relevance-mse}. No mais, as duas
figuras são bastante compatíveis e indicam uma tendência importante: a
minimização do EMQ também maximizará o produto SP. Esta figura também
indicaria que a ordem na qual o EGammaHypo considera as variáveis de corte
(i.e. $\rcore \rightarrow
\eratio \rightarrow \etem \rightarrow \ethad$) seja não-ótima para o conjunto
de dados sendo analisado, mesmo que marginalmente. Observa-se que seria mais
interessante que o corte em $\etem$ fosse realizado por último ao invés do
corte em $\ethad$, pois desta forma o processo discriminatório seria mais
rápido.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{lms/relevance-sp}
\end{center}
\caption{Os valores de relevância de discriminação para os conjunto de treino
(cinza claro) e teste (em cinza escuro), para as 4 variáveis do T2Calo e
considerando-se o classificador LMS em estudo.}
\label{fig:relevance-sp}
\end{figure}

\section{Métodos neurais de discriminação}
\label{sec:neural}

Redes neurais artificiais (RNA) vêm sendo atualmente utilizadas com sucesso em
problemas de otimização, controle e reconhecimento de padrões
\cite{haykin}. RNA's são modelos matemáticos inspirados no conhecimento
limitado que temos do cérebro animal. Neste contexto, entende-se que uma rede
neural pode aprender através do contato com os elementos de interesse de um
determinado espaço de entrada e que este conhecimento é armazenado através das
conexões sinápticas que conectam os elementos processadores (neurônios) da
rede. Dentre as características de uma RNA podemos destacar:

\begin{description}
\item[Robustez] Em ambientes extremamente \emph{agressivos} (sujeitos a falhas
e a radioatividade), como é o caso da operação do ATLAS, RNA's podem manter um
excelente desempenho, mesmo quando parte dos dados (canais de aquisição) de
entrada são perdidos;

\item[Generalização] RNA's podem extrair a informação relevante escondida sob
uma grande quantidade de ruído;

\item[Deteção de novos fenômenos] RNA's podem detetar a ocorrência de novos
objetos de forma bastante eficaz. Isto é \underline{extremamente} importante
em ambientes que podem revelar resultados inesperados (nova física);

\item[Simples implementação] RNA's podem ser facilmente descritas em
linguagens de programação convencionais, atingindo desempenhos satisfatórios
para um grande conjunto de aplicações em tempo real.
\end{description}

A utilização de RNA's em experimentos em Física de Altas Energias também vem
se popularizando ao longo das últimas década, principalmente na área de
calorimetria, tanto especificamente em sistemas de filtragem \cite{badgett92,
koehne96} quanto para análise pós-reconstrução \cite{altherr90}. Isto se deve
principalmente ao fato que RNA's consigam definir uma superfície de separação
não-linear no espaço de dados sendo considerado, normalmente atingindo
resultados bastante acurados na determinação do \eng{background} e ainda
retendo um desempenho satisfatório quando comparadas às técnicas de deteção
por cortes como as discutidas anteriormente (veja a
Seção~\ref{sec:def-eghypo}).

\subsection{Introdução ao processamento neural}

A Figura~\ref{fig:neuron} contém uma modelagem de um neurônio
genérico. Analogamente ao sistema LMS descrito na Seção~\ref{sec:lms}, os
elementos processadores de uma rede neural, chamados neurônios ou percéptrons,
são ativados por um conjunto de entradas $\overrightarrow{x}$, somadas de
acordo com um sistema linear de ponderação $\overrightarrow{w}$. O diferencial
entre o sistema linear proposto anteriormente e um neurônio está na função de
ativação que conduz o campo induzido $v_p =
\overrightarrow{x}*\overrightarrow{w}_{p}^{T}$ à saída. Enquanto que
no caso do LMS utiliza-se a função identidade, i.e., $y_p = v_p$, no caso de
percéptrons, faz-se uso de uma função não-linear, como por exemplo a função
tangente hiperbólica:

\begin{figure}
\begin{center}
\includegraphics{neuron}
\end{center}
\caption{Grafo de fluxo de sinal de um neurônio artificial.}
\label{fig:neuron}
\end{figure}

\begin{equation}
tanh(v) = \frac{e^v - e^{-v}}{e^v + e^{-v}} = \frac{e^{2v} - 1}{e^{2v} + 1}
\label{eq:tanh}
\end{equation}

Ou a função logística (simplificada):

\begin{equation}
P(v) = \frac{1}{1 + e^{-v}}
\label{eq:logf}
\end{equation}

A razão da escolha de uma função não-linear para a ativação de um campo
induzido pode ser qualificado à partir da seguinte constatação: se sistema a
que se deseja classificar aprensenta um comportamento gaussiano, para ambas as
classes, i.e., os momentos de ordem superior a $2$ para ambas as classes são
todos iguais a zero, o classificador ótimo (bayesiano) resume-se um sistema
linear \cite{haykin}. Naturalmente nota-se que seja sempre possível
\textit{aproximar} ou modelar um sistema não-gaussiano em um desta espécie,
tendo por conseqüência os erros relativos a esta aproximação. De fato, foi o
que foi realizado na Seção~\ref{sec:lms-ej}, quando escolhou-se utilizar um
classificador linear para separar as quatro variáveis definidas pelo T2Calo.

Se o sistema que se deseja resolver apresenta um comportamento não-gaussiano,
o classificador ótimo não pode ser representando por um classificador
linear. Neste caso, utiliza-se uma função de ativação não-linear para que se
aproxime, de alguma forma, o comportamento não-linear do sistema de interesse.
As funções nas Equações~(\ref{eq:tanh}) e (\ref{eq:logf}) são normalmente
utilizadas por apresentarem amplitude limitada e possuírem derivada
trivialmente calculável. A razão de procurar-se funções com derivadas simples
(e suaves) ficará mais clara adiante, quando se definir o algoritmo de
treinamento. A amplitude limitada pode rapidamente ser verificada como uma
característica de interesse observando-se que o treinamento de um sistema que
trabalha por aprendizado seja muitas vezes executado com a retro-alimentação
de erros. Neste caso, para evitar oscilações e pontos de ressonância, um
sistema cuja a saída seja limitada apresenta natural vantagem se comparado a
outro análogo \cite{haykin}.

Este estudo está focado na utilização de RNA's utilizando percéptrons em
múltiplas camadas (do inglês \eng{Multi-layer perceptrons} ou MLP),
completamente conectadas, sem realimentação e com treinamento baseado na
retropropagação de erros. Introduziremos o processo de treinamento destes
sistemas à seguir.

\subsection{Treinamento por retro-propagação de erros}

Para sistemas com apenas uma camada neural, observa-se que, ao buscar-se o
ponto de mínimo da superfície de erro (quadrático), naturalmente otimiza-se o
conjunto de pesos de tal forma que o sistema consiga mapear a entrada nos
alvos de saída. A única restrição do método é que a função de ativação do
campo induzido do neurônio $\varphi(\cdot)$ seja diferenciável.
\cite{rosenblatt}. O problema do treinamento de uma rede MLP é um pouco mais
complexo. Um grafo de fluxo de uma rede MLP completamente conectada pode ser
visto na Figura~\ref{fig:simple-mlp}. Neste caso considera-se que a rede
possui apenas uma camada escondida e apenas um neurônio de saída pois se
identifica intimamente com os casos de uso que abordaremos. Ainda sim, seria
possível considerar casos com um número maior de neurônios de saída ou com
mais camadas escondidas generalizando os casos exemplificados aqui.

\begin{figure}
\begin{center}
\includegraphics{simple-mlp}
\end{center}
\caption{Modelagem de uma rede MLP, totalmente conectada e sem
retro-propagação de sinal.}
\label{fig:simple-mlp}
\end{figure}

Para o sistema em questão, o mecanismo de treinamento para o neurônio de saída
é idêntico ao treinamento de um percéptron simples, e pode ser facilmente
definido da seguinte forma:

\begin{align}
\text{Se define-se o erro como } \mathcal{E}(n) &= \frac{1}{2}e_{k}^{2}(n)
\label{eq:error-def} \\
\text{e considerando-se que } \frac{\partial\mathcal{E}(n)}{\partial
w_{jk}(n)} &= \frac{\partial\mathcal{E}(n)}{\partial e_k(n)} \frac{\partial
e_k(n)}{\partial y_k(n)} \frac{\partial y_k(n)}{\partial v_k(n)}
\frac{\partial v_k(n)}{\partial w_{jk}(n)} \label{eq:partials} \\
\text{ou seja, } \frac{\partial\mathcal{E}(n)}{\partial
w_{jk}(n)} &= e_k(n)(-1)\varphi_{k}'(v_{k}(n))y_{k}(n)
\label{eq:partials-solution} \\
\text{e, resumindo, } \Delta w_{jk}(n) &=
-\eta\frac{\partial\mathcal{E}(n)}{\partial w_{jk}(n)} = \eta
e_{k}(n)\varphi_{k}'(v_{k}(n))y_{k}(n)
\end{align}

Normalmente, define-se:

\begin{align}
\text{Gradiente local: } \delta_k(n) &= - e_k(n)\varphi_{k}'(v_k(n)) \\
\text{e desta forma } \Delta w_{jk}(n) &= \eta\delta_k(n)y_{k}(n)
\label{eq:neural-train}
\end{align}

Este sistema de equações segue o princípio definido anteriormente para o LMS,
mas sem assumir nada sobre a função de ativação $\varphi(\cdot)$. Nota-se que
para uma função de ativação onde $y_k(n) = v_k(n)$, recai-se no algoritmo de
treinamento do LMS: $\Delta w_{jk}(n) = \eta e_{j}(n)y_{j}(n)$. Desta forma, é
possível considerar o LMS como um caso especial de uma rede neural com apenas
uma camada e cuja a função de ativação é a função identidade. Nestas equações,
$n$ representa a época ou batelada de treinamento.

O segundo caso de interesse acontece quando o neurônio $j$ está localizado em
uma camada oculta da rede. Neste caso, não existe uma resposta desejada (alvo)
específica para aquele neurônio. Desta forma, tentar-se-á definir o sinal de
erro de um neurônio escondido recursivamente, através da retro-propagação do
erro na saída final da rede em direção ao neurônio desejado. Iniciamos,
intuitivamente, com a definição do gradiente local do neurônio escondido $j$:

\begin{equation}
\delta_j(n) = -\frac{\partial\mathcal{E}(n)}{\partial
v_j(n)} = -\frac{\partial\mathcal{E}(n)}{\partial
y_j(n)}\frac{\partial y_j(n)}{\partial v_j(n)} =
-\frac{\partial\mathcal{E}(n)}{\partial y_j(n)}\varphi_{j}'(v_j(n))
\end{equation}

Tendo em conta, que para o caso específico em análise:

\begin{align}
\mathcal{E}(n) &= \frac{1}{2}e_{k}^{2}(n) \\
\text{conclui-se que } \frac{\partial\mathcal{E}(n)}{\partial y_j(n)} &=
e_{k}\frac{\partial e_k(n)}{\partial y_j(n)} = e_{k}\frac{\partial
e_k(n)}{\partial v_k(n)}\frac{\partial v_k(n)}{\partial y_j(n)}
\end{align}

A primeira derivada parcial, $\partial e_k(n)/\partial v_k(n)$, já foi
calculada para o caso do neurônio de saída, na passagem da
Equação~(\ref{eq:partials}) para a
Equação~(\ref{eq:partials-solution}). Utiliza-se a mesma analogia aqui. Para o
cálculo da segunda derivada parcial, pela Figura~\ref{fig:simple-mlp}
deduz-se que:

\begin{align}
v_k(n) &= \sum_{j} w_{jk}(n)y_j(n) \\
\text{e, portanto: } \frac{\partial v_k(n)}{\partial y_j(n)} &= w_{jk}(n)
\end{align}

Assim sendo, o gradiente local do neurônio escondido $j$ assim se define:

\begin{equation}
\delta_j(n) = \varphi_{j}'(v_j(n))\delta_k(n)w_{jk}(n)
\label{eq:local-gradient-hidden}
\end{equation}

Substituindo $\delta_j(n)$ na Equação~(\ref{eq:neural-train}), chega-se a
fórmula de treinamento do neurônio escondido. No caso onde há muitas saídas na
rede, a Equação~(\ref{eq:local-gradient-hidden}) é trivialmente redefinida da
seguinte forma: 

\begin{equation}
\delta_j(n) = \varphi_{j}'(v_j(n))\sum_{k}\delta_k(n)w_{jk}(n)
\label{eq:local-gradient-hidden-multi}
\end{equation}

No caso em que existem múltiplas camadas escondidas, propaga-se recursivamente
o erro em direção à camada de entrada, aplicando-se a
Equação~(\ref{eq:local-gradient-hidden-multi}). 

\subsubsection{Funções de ativação} 

A função de ativação $\varphi(\cdot)$ deve ser diferenciável em toda a
extensão do domínio de interesse. No entanto, para simplificar o cálculo
computacional dos gradientes locais, é habitual a escolha de funções que
possuam um cálculo trival de suas derivadas. Como exemplo, foram citadas as
funções tangente hiperbólica (Equação~\ref{eq:tanh}) e a função logística
simplificada (Equação~\ref{eq:logf}). No caso da função logística:

\begin{align}
\varphi(z) &= \frac{1}{1 + e^{-z}} \\
\text{Daí } \varphi'(z) &= \frac{e^{-z}}{[1+e^{-z}]^2}
\end{align}

Levando-se em consideração que para um neurônio genérico $y =
\varphi(v)$. Então:

\begin{equation}
\varphi'(v) = \frac{e^{-v}}{[1+e^{-v}]^2} = \frac{1+e^{-v}}{[1+e^{-v}]^2} -
\frac{1}{[1+e^{-v}]^2} = \varphi(v)[1 - \varphi(v)] = y(1-y)
\end{equation}

No caso da tangente hiperbólica, equivalentemente:

\begin{align}
\varphi(z) &= \tanh(z) \\
\varphi'(z) &= \text(sech)^2(z) = 1 - \tanh^2(z) \\
\varphi'(v(n)) &= 1 - \varphi^2(z) = 1-y^2(n)
\end{align}

Estas duas funções de ativação permitem um cálculo absolutamente trivial do
gradiente local e por esta razão são computacionalmente bastante
eficientes. No decorrer deste estudo utilizaremos a função tangente
hiperbólica com função de ativação dos percéptrons das redes estudadas.

\subsubsection{Convergência}

O algoritmo de retropropagação usa uma estimativa instantânea do gradiente da
superfície de erro no espaço dos pesos. Por esta razão, este sistema possui
uma inerente natureza estocástica e tende a oscilar ao redor da direção de
convergência ótima, ao mínimo da função de erro. Isso acontece pois é difícil
antever se inclinações demasiado bruscas ou suaves em uma das direções da
superfície de erro não influenciarão excessivamente o deslocamento do
sistema. Muitas vezes, a utilização de um amortecimento (ou \emph{momento})
durante o treinamento pode melhorar a resposta do sistema. Utiliza-se esta
técnica para suavizar a migração das redes em direção ao mínimo da superfície
de erro. Outra técnica é a normalização dos dados de entrada da rede, de forma
que se evite que a diferença de magnitude e variabilidade das componentes de
entrada cause tendências no treinamento neural.

Um problema recorrente no treinamento de redes neurais são mínimos locais, que
podem fazer com que o sistema fique ``preso'' em uma região que não represente
o mínimo global da superfície de erro. Esta característica não é somente mais
uma conseqüência da utilização da estimativa instantânea para definir a
direção de movimentação dos pesos, mas muitas vezes ocorre por dispor-se de
uma quantidade limitada de eventos que representem o fenômeno que se deseja
mapear.

Para que se assegure de que o sistema convirja sempre a um patamar equivalente
de mínimo, deve se realizar um número de experimentos com os mesmos parâmetros
de treinamento e teste para todos os casos de estudo. Em cada teste
inicializa-se os pesos sinápticos à partir de um ponto diferente da superfície
de erro. Desta forma, será possível detetar e avaliar se o problema em estudo
estará sujeito a mínimos locais.

\subsection{Discriminação neural aplicada às saídas do T2Calo}

É possível substituir o discriminador LMS definido na Seção{sec:lms} por um
discriminador baseado em uma rede neural tipo MLP como a da
Figura~\ref{fig:simple-mlp}. A implementação desta rede neural encontra-se bem
detalhada no Apêndice\ref{ap:framework} e é facilmente implementável usando
o pacote \eng{NeuralRinger} e o fluxo de treinamento detalhado na
Figura~\ref{fig:train-flow}. A Figura~\ref{fig:t2calo-neural} contém um
diagrama de blocos que ilustra, de forma simplificada, tal sistema. Os fatores
de normalização empregados para o sistema de deteção baseado no algoritmo LMS
não precisam ser substituídos, já que deseja-se empregar o mesmo tipo de
normalização antes de alimentarmos o sistema neural com os dados disponíveis,
a fim de evitar qualquer tipo de tendência devida à variância ou média dos
dados. Os conjuntos de treinamento e teste serão mantidos, o que simplificará
a comparação das duas técnicas.

\begin{figure}
\begin{center}
\includegraphics[scale=1]{t2calo-neural}
\end{center}
\caption{Um discriminador neural para as características definidas pelo
T2Calo.}
\label{fig:t2calo-neural}
\end{figure}

Neste novo sistema neural um conjunto maior de parâmetros de treinamento
estará disponível e torna-se necessária a avaliação destes em função do tempo
e qualidade de treinamento das redes. Estes parâmetros são:

\begin{itemize}
\item Taxa de aprendizagem da rede;
\item Tamanho da época;
\item Número de neurônios na camada escondida\footnote{Este estudo
limita-se a utilização de Redes Neurais artificiais com uma única camada
escondida.};
\item Momento.
\end{itemize}

A utilização de um valor de momento tem por objetivo amortecer o treinamento
de forma que a migração para o mínimo da superfície de erro ocorra de forma
suave, e será empregado no caso de observarmos que o treinamento comporta-se
de forma oscilatória. Para cada uma destas variáveis, define-se um conjunto de
sub-valores de interesse:

\begin{itemize}
\item Taxa de aprendizagem: $[0,001; 0,002; 0,005; 0,01; 0,02; 0,05; 0,1;
0,15]$;
\item Tamanho da época: $[50; 100; 200; 500; 1000; 1500]$;
\item Neurônios escondidos: $[2; 3; 4; 5; 6]$;
\item Momento: $[0; 0,01; 0,02; 0,05; 0,1; 0,2]$.
\end{itemize}

Cada conjunto individual de parâmetros deve ser avaliado em função de um
número considerável de pontos de inicialização dos pesos sinápticos da rede
neural. Deseja-se assegurar que as redes convirjam à pontos próximos (se não
idênticos) na superfície de erro considerada, independentemente de sua
inicialização. Estipulou-se que o número de testes para cada conjunto de
parâmetros será 5. Ou seja, para cada ponto no espaço de otimização do
treinamento, conduziremos 5 testes inicializando os pesos sinápticos com
valores aleatórios distintos e disparando o treinamento. Considerar-se-á os
máximos e mínimos obtidos no produto SP do classificador, para o conjunto de
teste, na avaliação de seu desempenho.

Uma inspeção rápida do número de testes a serem executados mostra que, para
este conjunto de parâmetros, cerca de ($8\times6\times5\times6\times5$) 
$7200$ testes devem ser realizados. Embora tentador, este cenário aparenta-se
mais complexo que o problema inicial da otimização do EGammaHypo, com uma
desvantagem: o número de parâmetros aumentou. Uma avaliação mais detalhada da
natureza do espaço de otimização pode ser benéfica a simplificação deste
processo: 

\begin{itemize}
\item Numa análise inicial, é possível suprimir a variação do momento, já que
a contribuição desejada deste parâmetro está no amortecimento do
treinamento. Seria possível adotar a seguinte estratégia: inicialmente
identifica-se a melhor taxa de aprendizagem. Em seguida, estuda-se como
aplicar o momento de forma que o discriminador final seja treinado evitando
transições abruptas do MSE ou do produto SP;

\item O número de neurônios escondidos poderia ser, inicialmente fixado em 4,
igualando o número de entradas. Reconhece-se que um sistema com menos
neurônios escondidos possa apresentar a mesma eficiência ou ainda que um
sistema com mais, convirja mais suavemente. Ainda sim, espera-se que as
tendências no tocante aos outros parâmetros de otimização sejam preservadas e
seja possível identificar o número ótimo de neurônios na camada escondida de
forma \textit{quasi}-independente ao restante dos valores;

\item O tamanho da época pode ser fixado baseando-se na experiência inicial
com o discriminador LMS. O valor utilizado para aquele sistema foi de 500
eventos por época.
\end{itemize}

Desta forma, propõe-se a seguinte heurística de otimização:

\begin{enumerate}
\item Fixa-se todos os parâmetros de treinamento, variando-se a taxa de
aprendizagem em busca de um ótimo para este parâmetro;
\item De posse da taxa de aprendizagem ótima, varia-se o tamanho da época;
\item O número de neurônios escondidos ótimo poderá então ser analisado tendo
em consideração valores ótimos de taxa de aprendizado e o tamanho da época;
\item Por final, varia-se o momento afim de suavizar ao máximo o treinamento
do detetor neural. A utilização de momento será necessária, \textbf{se e
somente se} o treinamento neural se mostrar excessivamente ruidoso.
\end{enumerate}

\subsubsection{Otimização da taxa de aprendizado}

O primeiro parâmetro a ser otimizado, seguindo a heurística proposta é a taxa
de treinamento. Neste caso, 8 valores estão sendo considerados. Para cada um
dos 8 valores, realizou-se 5 testes onde os pesos sinápticos são inicializados
de forma aleatória e distinta. Para cada conjunto de testes relacionados com
um valor fixo de taxa de treinamento, encontra-se a média e o desvio padrão do
produto SP para os conjuntos de treino e teste e do número de passos de
treinamento. Os resultados desta análise podem ser vistos na
Tabela~\ref{tab:t2calo-neural-lr-scan}. O tamanho da época está fixo em 500, o
que indica que, em todos os casos, o sistema teve acesso a um número maior de
padrões que o total disponível para treinamento (cerca de $14500$ padrões,
incluindo elétrons e jatos).

\begin{table}
\begin{center}
\begin{tabular}{|r|r|r|r|} \hline
Tx. de aprendizado & Passos & Prod. SP (treino) & Prod. SP (teste) \\
\hline 
0.0010 & $1339\pm248$ & $1.4935\pm0.0162$ & $1.4943\pm0.0178$ \\ \hline
0.0020 & $1048\pm407$ & $1.4850\pm0.0108$ & $1.4880\pm0.0107$ \\ \hline
0.0050 & $617\pm229$ & $1.4995\pm0.0163$ & $1.4932\pm0.0147$ \\ \hline
0.0100 & $335\pm51$ & $1.4943\pm0.0074$ & $1.4918\pm0.0059$ \\ \hline
0.0200 & $204\pm36$ & $1.5168\pm0.0127$ & $1.5104\pm0.0139$ \\ \hline
0.0500 & $123\pm21$ & $1.5087\pm0.0123$ & $1.5052\pm0.0134$ \\ \hline
0.1000 & $80\pm21$ & $1.5150\pm0.0067$ & $1.5101\pm0.0055$ \\ \hline
0.1500 & $85\pm28$ & $1.5121\pm0.0046$ & $1.5094\pm0.0032$ \\ \hline
\end{tabular}
\end{center}
\caption{Resultados da otimização da taxa de treinamento para um discriminador
neural elétron/jato baseado na saída do T2Calo.}
\label{tab:t2calo-neural-lr-scan}
\end{table}

A tabela mostra que um valor pequeno de taxa de treinamento faz com que a
duração deste processo se extenda demasiado sem, necessariamente compensar em
melhores resultados. Em todos os casos, o sistema parece sempre convergir para
um máximo próximo à 1,50 de produto SP (considerando valores para o conjunto
de teste). Conforme aumenta-se a taxa de treinamento, o número de passos para
a estabilização do sistema diminui, chegando, no caso em que a taxa é igual a
$0,1$, a necessitar apenas de 80 iterações para a convergência. Uma vez que a
média do produto SP apresenta-se maior para o valor de taxa de treinamento
igual a $0,02$, utilizaremos este valor como o valor padrão nas próximas
etapas de otimização.

\subsubsection{Otimização do tamanho da época}

De posse da taxa de aprendizado, é a vez de otimizar-se o tamanho da época,
inicialmente fixa em 500. Os valores de época para o qual deseja-se
experimentar são: $[50; 100; 200; 500; 1000; 1500]$. O mesmo procedimento
utilizado para a otimização da taxa de treinamento será realizado aqui: cinco
testes são realizados para cada tamanho de época. Os resultados destes testes
encontram-se na Tabela~\ref{tab:t2calo-neural-epoch-scan}.

\begin{table}
\begin{center}
\begin{tabular}{|r|r|r|r|} \hline
Padrões por época & Passos & Prod. SP (treino) & Prod. SP (teste) \\
\hline 
50 & $198\pm44$ & $1.5128\pm0.0190$ & $1.5056\pm0.0168$ \\ \hline
100 & $201\pm53$ & $1.5126\pm0.0070$ & $1.5128\pm0.0138$ \\ \hline
200 & $219\pm65$ & $1.5017\pm0.0100$ & $1.5004\pm0.0129$ \\ \hline
500 & $204\pm36$ & $1.5168\pm0.0127$ & $1.5104\pm0.0139$ \\ \hline
1000 & $212\pm34$ & $1.5202\pm0.0167$ & $1.5188\pm0.0121$ \\ \hline
1500 & $185\pm52$ & $1.5083\pm0.0142$ & $1.5051\pm0.0184$ \\ \hline
\end{tabular}
\end{center}
\caption{Resultados da otimização do tamanho da época para um discriminador
neural elétron/jato baseado na saída do T2Calo.}
\label{tab:t2calo-neural-epoch-scan}
\end{table}

Como mostra a tabela, um tamanho de época de $1000$ padrões maximiza a
capacidade discriminante do sistema. O número de iterações não varia
significativamente o que indica que o tempo de treinamento do sistema não está
diretamente correlacionado ao tamanho da época. Utilizar-se-á o valor de
$1000$ como refência na próxima etapa de otimização.

\subsubsection{Otimização do número de neurônios escondidos}

O número de neurônios na camada escondida é a penúltima ou, possivelmente
última etapa na otimização do classificador neural proposto. Neste caso,
utilizaremos os valores pré-estipulados para os dois outros parâmetros: taxa
de treinamento $=0,02$ e tamanho da época de $1000$ padrões. Os valores de
teste são 2, 3, 4, 5 ou 6 neurônios na camada escondida. Mais uma vez, 5
testes são realizados para cada parâmetro e a média e variância do tempo de
treinamento e dos produtos SP é extraída. A
Tabela~\ref{tab:t2calo-neural-hidden-scan} contém os resultados destes testes.

\begin{table}
\begin{center}
\begin{tabular}{|r|r|r|r|} \hline
Número de neur. escondidos & Passos & Prod. SP (treino) &
Prod. SP (teste) \\ \hline 
2 & $259\pm86$ & $1.5086\pm0.0104$ & $1.5078\pm0.0118$ \\ \hline
3 & $178\pm21$ & $1.4967\pm0.0039$ & $1.4921\pm0.0043$ \\ \hline
4 & $212\pm34$ & $1.5202\pm0.0167$ & $1.5188\pm0.0121$ \\ \hline
5 & $173\pm25$ & $1.4982\pm0.0058$ & $1.4941\pm0.0046$ \\ \hline
6 & $198\pm98$ & $1.5025\pm0.0121$ & $1.4986\pm0.0168$ \\ \hline
\end{tabular}
\end{center}
\caption{Resultados da otimização do número de neurônios na camada escondida
para um discriminador neural elétron/jato baseado na saída do T2Calo.}
\label{tab:t2calo-neural-hidden-scan}
\end{table}

Desta tabela, concluímos que o número ótimo de neurônios na camada escondida
deve ser 4. Uma inspeção no desenvolvimento do EMQ e do produto SP nos testes
com os 3 parâmetros de escolha indica que a migração para o mínimo seja suave
e, portanto não seja necessário a aplicação de momento ou qualquer outra
técnica de suavização do treinamento. Em seguida, para confirmar as escolhas,
treina-se um total de 10 redes com os parâmetros selecionados. Os critérios de
parada utilizados são mantidos. Para nos assegurarmos da estabilidade da rede,
deixa-se que a rede treine ainda por cerca de 100 passos depois do ponto de
estabilidade detetado. A Figura~\ref{fig:best-t2calo-mlp-mse} e a
Figura~\ref{fig:best-t2calo-mlp-sp} mostram respectivamente o desenvolvimento,
ao longo do treinamento dos valores EMQ e produto SP para o melhor dos 10
resultados obtidos.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/mse-evolution}
\end{center}
\caption{Evolução do EMQ para os conjuntos de treino e teste de um
discriminador elétron/jato neural para as variáveis do T2Calo.}
\label{fig:best-t2calo-mlp-mse}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/sp-evolution}
\end{center}
\caption{Evolução do produto SP para os conjuntos de treino e teste de um
discriminador elétron/jato neural para as variáveis do T2Calo.}
\label{fig:best-t2calo-mlp-sp}
\end{figure}

O sistema estabiliza após cerca de 300 passos, considerando a evolução do
EMQ. No que tange capacidade discriminante, aferida através do produto SP,
após cerca de 100 passos, o sistema parece se tornar irrelevante à contínua
redução do EMQ. A Figura~\ref{fig:best-t2calo-mlp-output} mostra as saídas da
rede, no final do treinamento, para elétrons (em cima) e jatos (em baixo). A
saída para elétrons é fixada em $-1$ e para jatos, em $+1$. Nota-se que,
diferentemente ao sistema LMS, há uma notável separação entre as classes
proporcionada pelo emprego de um discriminador neural. O ponto ótimo de
separação pode ser mais facilmente determinado, inclusive por inspeção
visual. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/test-output}
\end{center}
\caption{Saída para o conjunto de teste, do discriminador neural baseado nas
características extraídas pelo T2Calo.}
\label{fig:best-t2calo-mlp-output}
\end{figure}

Diferentemente do sistema linear baseado no LMS, uma rede neural poderá traçar
uma superfície curva no espaço de entrada (número de dimensões $=4$),
melhorando a separação entre as classes que se deseja detetar. Esta última
figura mostra o resultado desta capacidade discriminativa. Ainda sim, como é
possível inspecionar na Figura~\ref{fig:best-t2calo-test-roc}, a capacidade
discriminante deste algoritmo neural resta bastante próxima à do sistema
baseado num detetor LMS, chegando, em alguns pontos da R.O.C. a ser inferior.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/mlp-vs-lms-vs-egamma-roc}
\end{center}
\caption{R.O.C. para o conjunto de teste, do discriminador neural baseado nas
características extraídas pelo T2Calo, comparado com o detetor baseado no LMS
e a técnica de otimização baseado algoritmo EGammaHypo.}
\label{fig:best-t2calo-test-roc}
\end{figure}

O valor do produto SP máximo na curva para o detetor neural está nas
coordenadas onde a deteção de elétrons é igual a $93,25\%$ e o falso-alarme em
jatos é $88,79\%$ ($\approx 2,80$~kHz de falso-alarme). Estes valores
correspondem a um produto SP de $1,51$, tal qual ao caso do discriminador
LMS. Uma verificação das eficiências parcias por região em $\eta$ e em $\phi$
revelam uma estrutura bastante semelhante aos sistemas de deteção precedentes,
como mostram as Figuras~\ref{fig:best-t2calo-test-sp-eta} e
\ref{fig:best-t2calo-test-sp-phi}. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/test-sp-eta}
\end{center}
\caption{Análise do produto SP ao longo de $\eta$ para o discriminador neural
baseado nas saídas do T2Calo.}
\label{fig:best-t2calo-test-sp-eta}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/test-sp-phi}
\end{center}
\caption{Análise do produto SP ao longo de $\phi$ para o discriminador neural
baseado nas saídas do T2Calo.}
\label{fig:best-t2calo-test-sp-phi}
\end{figure}

A Figura~\ref{fig:best-t2calo-test-sp-emet} mostra o valor relativo do produto
SP for faixa de energia transversa na seção e.m.. Um comparativo entre os 3
métodos do produto SP por faixas de energia descritos até agora se segue
na Figura~\ref{fig:best-t2calo-versus-others-emet}. Os dados nesta figura
correspondem aos do conjunto de teste, para um corte efetuado em $-0.273$ para
o discriminador neural, e para os melhores resultados obtidos com os outros
dois sistemas de deteção de forma equivalente.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/test-sp-emet}
\end{center}
\caption{Análise do produto SP ao longo de $E^{e.m.}_T$ para o discriminador
neural baseado nas saídas do T2Calo.}
\label{fig:best-t2calo-test-sp-emet}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/mlp-vs-lms-vs-egamma-et}
\end{center}
\caption{Comparativo do produto SP para os 3 detetores abordados até aqui, por
faixa de energia e.m. transversa.}
\label{fig:best-t2calo-versus-others-emet}
\end{figure}

É possível notar que o sistema neural ganha nas faixas energéticas onde há
mais eventos, porém apresenta uma eficiência marginalmente mais baixa conforme
a energia na seção e.m. aumenta. Num geral, os três sistemas possuem
características bastante próximas nesta análise, concordando entre si. As
análises de relevância, tanto baseadas no EMQ quanto no valor máximo do
produto SP encontram-se nas Figuras~\ref{fig:best-t2calo-mse-relevance} e
\ref{fig:best-t2calo-sp-relevance}. A primeira análise (EMQ) nota-se que há
concordãncia, resguardadas as diferenças nos padrões de saída em cada um dos
casos, na relevância de cada característica neste quesito. O método neural, no
entanto, acentua a falta de importância da componente hadrônica na saída da
rede. Na análise de relevância baseada no máximo do produto SP nota-se que a
ordem de relevância está alterada. Na primeira análise, a variável $\rcore$
parece ser a mais importante, seguindo-se das variáveis $\eratio$, $\ethad$ e
finalmente $\etem$. Baseando-se na variação do produto SP, a variável mais
importante na discriminação é $\eratio$, seguindo-se então de $\rcore$,
$\ethad$ e $\etem$. Diferentemente do sistema linear, as três mais importantes
variáveis de discriminação possuem valores de relevância relativamente
próximos (nos arredores de $0,10$ para $\etem$, $0,15$ para $\rcore$ e $0,20$
para $\eratio$). No sistema linear, a relevância na deteção é fortemente
encabeçada pela variável $\rcore$, com as outras variáveis cerca de uma ordem
de magnitude menos relevantes. 

Um outro ponto observável na análise baseada na variação do produto SP é que,
para o conjunto de treino, a variável $\etem$ possui relevância de
discriminação negativa, encontrando-se aproximadamente em $-0,005$. Este
resultado indica que esta variável estaria atrapalhando o processo de deteção.
Porém, uma vez que seu valor homólogo para o conjunto de teste tenha valor
positivo, nas mesmas proporções, assume-se que este resultado advém,
possivelmente, de uma flutuação estatística e não representa uma real
tendência da variável em questão. Observando os valores de relevância baseados
na diferença do produto SP para os outros 9 testes realizados para o mesmo
conjunto de parâmetros de treinamento, nota-se que esta tendência repete-se em
apenas alguns dos testes, reforçando a conclusão de que este resultado se
trataria apenas de uma flutuação estatística.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/relevance-mse}
\end{center}
\caption{Os valores de relevância para os conjunto de treino
(cinza claro) e teste (em cinza escuro), para as 4 variáveis do T2Calo e
considerando-se o classificador neural em estudo.}
\label{fig:best-t2calo-mse-relevance}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{t2calo-mlp/relevance-sp}
\end{center}
\caption{Os valores de relevância de discriminação para os conjunto de treino
(cinza claro) e teste (em cinza escuro), para as 4 variáveis do T2Calo e
considerando-se o classificador neural em estudo.}
\label{fig:best-t2calo-sp-relevance}
\end{figure}

Estas análises também indicam que, tendo por base conjunto de variáveis do
T2Calo, as classes de elétrons e jatos sejam linearmente separáveis. Este
resultado é bastante natural, dado que este conjunto de características tenha
sido otimizado para uma análise baseada em cortes definidos por um operador
experimentado. A Figura~\ref{fig:t2calo-mlp-best-net} mostra uma representação
discriminador neural discutido até agora, para fins ilustrativos. Na próxima
seção definiremos um novo tipo de extração de características que terá por
objetivo maximizar a deteção de elétrons e jatos.

\begin{figure}
\begin{center}
\begin{sideways}
\includegraphics[scale=0.30]{t2calo-mlp/mlp-end}
\end{sideways}
\end{center}
\caption{Diagrama de fluxo do sistema de deteção elétron/jato, neural, usando as
características do T2Calo como entrada.}
\label{fig:t2calo-mlp-best-net}
\end{figure}

\section{Mapeamento topológico}

O algoritmo T2Calo, durante a análise conduzida no LVL2, compacta a informação
da RoI representando um candidato à elétron em quatro quantidades altamente
discrimantes. Tendo estes valores por base obteve-se os seguintes resultados:

\begin{enumerate}
\item Para o EGammaHypo, $91,85\%$ de eficiência na deteção de elétrons contra
$10,19\%$ de falso alarme em jatos;
\item Para detetor LMS, $91,64\%$ para elétrons contra $9,35\%$ de falso alarme;
\item Para uma rede neural, $93,25\%$ para elétrons contra $11,21\%$ de falso
alarme.
\end{enumerate}

Embora as técnicas (LMS e MLP) propostas sejam mais robustas quanto à operação
e manutenção, a qualidade física da separação não é tão afetada. Isto se deve,
principalmente, à agressiva compressão nos dados proporcionada pelo algoritmo
de extração de características. Ainda que não seja viável a deteção de objetos
usando toda a informação da RoI (cerca de 1300 células), é possível encontrar
um método que se situe a meio caminho entre este dois extremos.

Nesta seção, propõe-se um método de compactação da informação da RoI que
utiliza a topologia do evento para gerar um conjunto de características que
pode ser usado para atingir maiores eficiências de deteção de elétrons e
menores valores de falso-alarme.

\subsection{Topologia de uma RoI e anelamento}

Observando-se a interação de elétrons e jatos (enquanto falsos-elétrons) com
os calorímetros, é possível assumir que estes objetos interagirão de forma
aproximadamente isotrópica em relação ao eixo $\eta$ do detetor. Ou seja, dado
um ponto de impacto inicial, a partícula tende a decair em objetos menos
energéticos ao redor do eixo de penetração. Este fenômeno ocorrerá de forma
expansiva, formando finalmente um cone de deposição energética, como o
exemplificado na Figura~\ref{fig:cone}.

\begin{figure}
\begin{center}
\includegraphics{msc/cone}
\end{center}
\caption{Modelo de um objeto e.m. interagindo com um calorímetro.}
\label{fig:cone}
\end{figure}

A Figura~\ref{fig:electron-roi} mostra um gráfico com a deposição energética na
segunda camada e.m. de uma RoI proveniente de um elétron simulado. As partes
acinzentadas indicam deposição energética, enquanto que as partes em branco
indicam que houve pouca ou nenhuma deposição energética na região.

\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{roi-em2}
\end{center}
\caption{Imagem tri-dimensional mostrando o padrão de deposição energética de
um elétron simulado na segunda camada e.m. do calorímetro do ATLAS.}
\label{fig:electron-roi}
\end{figure}

Como colocado na Seção~\ref{sec:e-detection}, a topologia da interação do
objeto em análise com os calorímetros define, com boa margem de sucesso o tipo
do mesmo. Elétrons tendem a um menor espalhamento que jatos, percorrendo
menores distâncias ao longo do eixo $\eta$. Por outro lado, jatos tendem a
radializar a deposição energética, formando cones mais largos e possivelmente
mais profundos.

Levando-se em consideração a formatação dos calorímetros do ATLAS e sua
granularidade, sugere-se o seguinte algoritmo de compactação:

\begin{description}
\item[\ding{182}] Define-se, para cada camada, o centro de posição
energética. Isto é feito de forma simplificada, procurando para todas as
células dentro de uma mesma camada, aquela célula com maior deposição
energética;
\item[\ding{183}] Uma vez que o centro de interação esteja definido, formar um
conjunto de anéis com formato retangular que circumdem este pico. A largura
destes anéis é ajustada de forma que a largura do anel de energia seja a mesma
que de uma célula dentro da camada estudada;
\item[\ding{184}] Para cada anel, somar os valores energéticos das células que
recaem sobre o seu interior.
\end{description}

A Figura~\ref{fig:rings} esquematiza tal algoritmo. Nesta figura encontram-se
exemplos de anéis de energia formados em 4 diferentes camadas dos calorímetros
do detetor ATLAS. Próximo ao centro da RoI, nota-se uma célula que destaca,
hipoteticamente, o centro de deposição energética que a etapa \ding{182} do
algoritmo encontraria. Em seguida, as células de cada segmento são somadas
para a obtenção das ``características'' do objeto a ser discriminado.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{msc/rings}
\end{center}
\caption{Esquematização do processo de anelamento para as diferentes camadas
dos calorímetros do ATLAS.}
\label{fig:rings}
\end{figure}

A granularidade dos anéis respeita a granularidade \emph{típica} das células
na camada. Sabe-se, no entanto, que a granularidade das células varia com
$\eta$ no detetor. Nos casos onde a granularidade é diferente da granularidade
padrão na camada, atribuir-se-á a energia da célula ao anel onde recai o
centro da mesma. Este algoritmo também é resiliente à dados
faltantes\footnote{Ocorrendo, potencialmente, com a falha de um canal de
leitura ou do sistema de aquisição de dados.}, uma vez que células que não
puderem ser lidas apenas não serão contabilizadas. Isto evita procedimentos de
verificação especiais que poderiam comprometer o desempenho do algoritmo de
compactação proposto. Através desta figura observa-se igualmente que algumas
camadas, devido ao formato não simétrico das células (maiores numa direção que
na outra), terão anéis abertos ou incompletos. Desta forma, marcou-se
segmentos que pertencem ao mesmo anel com um número indicando a que anel
determinado segmento pertence.

Um problema recorrente nos algoritmos de filtragem do ATLAS é a região de
\eng{wrap-around} da variável $\phi$, já que o detetor tem formato
cilíndrico. Um cuidado especial deve ser tomado na confecção dos anéis,
levando-se em consideração que, por exemplo, a região com $\phi = 6,18$ está
bastante próxima a região com $\phi = 0,1$, apesar da diferença
numérica. Outro problema encontrado é que diferentes partes do sistema de
localização dos dados dentro do Athena identificam a região do detetor onde
$\phi > \pi$ de maneiras distintas. Para parte do código (relativa à seção
e.m.), o detetor se encontra na área entre $[-\pi, \pi]$ enquanto que para
outra (parte hadrônica), o detetor se encontra na área entre $[0, 2\pi]$. Para
resolver este problema, implementou-se um algoritmo que normaliza as
diferenças em ambos os casos.

Ademais, por se tratar de elétrons ou jatos na qualidade de falsos-elétrons, é
bastante provável que a relação sinal-ruído seja pequena nas camadas traseiras
do detetor, principalmente na região hadrônica. A Figura~\ref{fig:roi-had-9}
mostra a deposição energética de um elétron na primeira camada do detetor
hardrônico. Nesta figura este fenômeno pode ser bem observado.

\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{roi-had-9}
\end{center}
\caption{Imagem tri-dimensional mostrando o padrão de deposição energética de
um elétron simulado na primeria camada hadrônica do calorímetro do ATLAS.}
\label{fig:roi-had-9}
\end{figure}

Para evitar que o ruído inerente à eletrônica do detetor possa interferir com
o aferimento do centro de deposição energética, a procura do pico em uma
camada está restrita a uma região de $\eta=0,1\times\phi=0,1$ ao redor do pico
de deposição na segunda camada e.m.. Como já colocado um capítulo anterior
deste trabalho, a segunda camada e.m. é a mais profunda do sistema de
calorimetria, concentrando a maior parte da energia dos elétrons que interagem
com este detetor. Desta forma, se há um pico de deposição energética, este é
considerado para a formação dos anéis. De outra maneira, um ponto aleatório
(aquele com maior energia) dentro da sub-janela definida pelo pico de
deposição energética na segunda camada e.m. é escolhido.

Tomando-se em consideração a granularidade padrão de todas as camadas do
calorímetro (7 no total, incluindo o pré-irradiador), a configuração sugerida
para o número de anéis pode ser encontrada na Tabela~\ref{tab:ring-config}. O
número de anéis em cada camada leva em consideração o número máximo de anéis
que podem ser formados considerando-se uma sub-região de $0,4 \times 0,4$ no
plano $\eta\times\phi$. É interessante notar que nem sempre esta região
cobrirá completamente uma região do mesmo tamanho, já que o posicionamento dos
anéis depende do centro de deposição energética em cada camada. No entanto,
como podemos ver no exemplo do T2Calo, uma região de $0,3 \times 0,3$ está
garantida ao redor do pico de energia, o que é suficiente para a análise de
elétrons \cite{daqnote00-02}.

\begin{table}
\begin{center}
\begin{tabular}{>{\bfseries}l r r r}
Camada & Número de Anéis & Larg. do anel ($\eta$) & Larg. do anel ($\phi$) \\ \hline
Pré-irradiador & 8 & $0,025$ & $\frac{\pi}{32} \approx 0,098174$ \\ 
1\eira\ e.m. & 64 & $0,0031245$ & $\frac{\pi}{32} \approx 0,098174$ \\ 
2\eira\ e.m. & 8 & $0,025$ & $\frac{\pi}{128} \approx 0,024544$ \\ 
3\eira\ e.m. & 8 & $0,05$ & $\frac{\pi}{128} \approx 0,024544$ \\ 
1\eira\ hadrônica & 4 & $0,1$ & $\frac{\pi}{32} \approx 0,098174$ \\ 
2\eira\ hadrônica & 4 & $0,1$ & $\frac{\pi}{32} \approx 0,098174$ \\ 
3\eira\ hadrônica & 4 & $0,1$ & $\frac{\pi}{32} \approx 0,098174$ \\ \hline
Total & 100 & & \\ \hline
\end{tabular}
\end{center}
\caption{Configuração para os anéis formados à partir de uma RoI de tamanho
$\eta = 0,4 \times \phi = 0,4$.}
\label{tab:ring-config}
\end{table}

Um total de $100$ anéis forma o conjunto de entrada para o novo sistema de
discriminação elétron/jato. Nota-se que o primeiro anel de cada conjunto será
considerado o pico de deposição energética na camada. Para classificar os
padrões neste espaço de alta dimensionalidade, utilizaremos detetores baseados
em redes neurais tipo MLP como descrito nas seções anteriores.

\subsubsection{Normalização}

Uma vez que estaremos lidando com os diferenciais energéticos dentro da mesma
camada, por muitas vezes sutis, faz-se necessário que a normalização aplicada
aos conjuntos de anéis seja compatível com a deteção destas
diferenças. Ademais, outros estudos \cite{seixas:pca} já demonstraram que a
informação periférica dentro de cada camada de uma RoI é tão ou mais
importante à discriminação que a quantidade de energia depositada no pico. Por
estas razões, neste trabalho propomos um sistema de normalização dos dados da
RoI baseado em um conjunto de razões cujo o denominador diminui, de forma
relacionada a energia na camada, do centro para as bordas da região de
interesse. Desta forma, o pico de deposição energética na camada (ou
``primeiro anel'') teria um fator de normalização maior que o segundo anel,
que seria por sua vez maior que o fator de normalização aplicado ao terceiro
anel e, assim, sucessivamente. O algoritmo está descrito matematicamente na
Tabela~\ref{tab:seq}.

\begin{table}
\renewcommand{\baselinestretch}{1}
\begin{center}
\begin{tabular}{>{\bfseries}l r r}
Anel & Normalização & Soma em anel \\ \hline
1 & $E$ (total da camada) & $E_1$\\
2 & $E - E_1$ & $E_2$ \\
3 & $E - E_1 - E_2$ & $E_3$\\
4 & $E - E_1 - E_2 - E_3$ & $E_4$\\
\dots  & \dots \\
N-4 & $E - E_1 - E_2 - ... - E_{N-5}$ & $E_{N-4}$ \\
N-3 & $E - E_1 - E_2 - ... - E_{N-5} - E_{N-4}$ & $E_{N-3}$ \\
N-2 & $E - E_1 - E_2 - ... - E_{N-5} - E_{N-4}$ & $E_{N-2}$ \\
N-1 & $E - E_1 - E_2 - ... - E_{N-5} - E_{N-4}$ & $E_{N-1}$ \\
N & $E - E_1 - E_2 - ... - E_{N-5} - E_{N-4}$ & $E_N$ \\
\end{tabular}
\end{center}
\renewcommand{\baselinestretch}{1.5}
\caption{Algoritmo para a ``Normalização Seqüencial''.}
\label{tab:seq}
\end{table}

Neste algoritmo, o primeiro anel (pico de deposição energética) será
normalizado pelo valor da energia total na camada. O segundo anel pelo valor
da camada subtraindo-se o valor de deposição energética no primeiro anel. O
terceiro anel, pelo valor da energia na camada menos o valor do primeiro e
segundo aneís e desta forma, recursivamente até que todos os anéis estejam
normalizados. Nos referiremos à este algoritmo à partir de agora como
algoritmo de ``Normalização Seqüencial''.

Nota-se que este tipo de normalização poderá amplificar em demasiado os
valores periféricos. Isto é desejável para os anéis que se sucedem ao pico de
energia na camada, porém nem tanto para os anéis da periferia da RoI, onde a
relação sinal-ruído é praticamente nula. Desta forma, propõe-se que, à partir
de um determinado limiar de energia, o fator de normalização torne-se
constante. Desta forma evita-se que o sinal ruidoso das bordas influencie o
sistema de discriminação de maneira significativa.

O sub-pacote \texttt{rbuild} do \eng{NeuralRinger} contém a implementação de
referência para este processo de anelamento descrito. Ademais, este pacote
pode ser re-configurado de forma a definir a largura dos aneis de cada camada
ou o número máximo de anéis por camada. Assim, o sistema de anelamento torna-se
independente do tamanho real da RoI e, portanto, independente de quão completo
são os dados recebidos pelo processador do LVL2. Anéis sem dados recebem o
valor \eng{default} que é igual a zero. O formato do arquivo de configuração é
também em XML, o que simplifica a leitura e escrita dentro do contexto do
projeto.

\subsubsection{Dados}

Os dados utilizados nas seções anteriores não podem ser re-utilizados para
esta porção do projeto, uma vez que é necessita-se do acesso aos dados
``brutos'' da RoI. Em outras palavras, deseja-se acesso aos mesmos dados
utilizados pelo T2Calo para a operação de extração de características que este
algoritmo realiza. De posse destes dados, é possível definir a operação de
compactção topológica proposta. A Figura~\ref{fig:datagen} mostra o sistema
que foi utilizado para a obtenção dos dados. Para cada RoI que é analisada
pelo T2Cal, um conjunto de células é carregado dos arquivos de dados
iniciais. Em seguida, os valores das células são calibrados e mapeados na
geometria do detetor de forma que possam ser mais facilmente acessados pelo
algoritmo de extração de características. À partir deste ponto, o T2Calo tem
acesso aos dados e, é exatamente neste ponto que inserimos um \eng{patch}, que
permite que os valores das células ``vistas'' pelo algoritmo sejam guardadas
em um arquivo separado.

\begin{figure}
\begin{center}
\includegraphics[scale=0.74]{datagen}
\end{center}
\caption{Estratégia para obter acesso aos dados tais quais processados pelo
algoritmo T2Calo.}
\label{fig:datagen}
\end{figure}

O formato deste arquivo é legível pelo sub-pacote \texttt{roiformat} do
\eng{NeuralRinger} e pode ser transformado em uma base de dados de anéis
utilizando-se o programa \texttt{ringer}, descrito no
Apêndice~\ref{ap:framework}. Desta forma, criou-se uma base de dados à partir
do mesmo conjunto de eventos analisados na caracterização do desempenho físico
do T2Calo. Esta nova base de dados tem cerca de $30000$ eventos com a mesma
proveniência e características dos eventos analisados até o momento.

\subsection{Discriminação baseada em anelamento}

O processo de anelamento reduz o espaço de dados de entrada em uma RoI (cerca
de 1300 células) para 100 valores correspondes aos anéis formados com os dados
das 7 camadas dos calorímetros. Sería difícil conceber um sistema de
discriminação simples como o proposto para o EGammaHypo neste caso, uma vez
que a possibilidade de combinações é muito grande. Neste caso, adota-se um
sistema de discriminação neural, conectado à saída do sistema de anelamento
que executará o processo de deteção.

Tal qual nos sistemas anteriores, executou-se uma rápida análise do espaço dos
parâmetros de treinamento neural. A Tabela~\ref{tab:ringer-param-optimization}
resume os resultados e indica os valores de taxa de treinamento, tamanho da
época e número de neurônios escondidos utilizados. Para cada tripla destes
parâmetros, 5 testes foram executados e o resultado exposto na tabela
representa a média e desvio padrões levando-se em consideração estas 5
instâncias.

\begin{table}
\begin{center}
\renewcommand{\baselinestretch}{1.0}
{\tiny
\begin{tabular}{|l|l|l|r|r|r|} \hline
\rottext{Tx. de Aprendizado} & \rottext{Tamanho da Época} & \rottext{Neurônios escondidos} & \rottext{Número de iterações} & \rottext{Produto SP (treino)} & \rottext{Produto SP (teste)} \\ \hline \hline

0.0100 & 100 & 4 & $356\pm78$ & $1.6329\pm0.0149$ & $1.6199\pm0.0156$ \\ \hline
0.0100 & 100 & 6 & $372\pm88$ & $1.6237\pm0.0247$ & $1.6202\pm0.0183$ \\ \hline
0.0100 & 100 & 8 & $280\pm39$ & $1.5987\pm0.0210$ & $1.6017\pm0.0215$ \\ \hline
0.0100 & 100 & 10 & $337\pm58$ & $1.6263\pm0.0385$ & $1.6137\pm0.0320$ \\ \hline
0.0100 & 200 & 4 & $366\pm43$ & $1.6119\pm0.0335$ & $1.5986\pm0.0302$ \\ \hline
0.0100 & 200 & 6 & $299\pm33$ & $1.5961\pm0.0236$ & $1.5945\pm0.0264$ \\ \hline
0.0100 & 200 & 8 & $340\pm62$ & $1.6246\pm0.0250$ & $1.6144\pm0.0221$ \\ \hline
0.0100 & 200 & 10 & $287\pm75$ & $1.5994\pm0.0444$ & $1.5968\pm0.0342$ \\ \hline
0.0100 & 500 & 4 & $285\pm64$ & $1.5914\pm0.0211$ & $1.5880\pm0.0225$ \\ \hline
0.0100 & 500 & 6 & $296\pm53$ & $1.6061\pm0.0251$ & $1.5998\pm0.0200$ \\ \hline
0.0100 & 500 & 8 & $254\pm56$ & $1.5981\pm0.0165$ & $1.5937\pm0.0156$ \\ \hline
0.0100 & 500 & 10 & $343\pm68$ & $1.6177\pm0.0273$ & $1.6120\pm0.0225$ \\ \hline
0.0100 & 1000 & 4 & $250\pm44$ & $1.5974\pm0.0215$ & $1.5897\pm0.0183$ \\ \hline
0.0100 & 1000 & 6 & $260\pm24$ & $1.5999\pm0.0307$ & $1.5998\pm0.0201$ \\ \hline
0.0100 & 1000 & 8 & $267\pm41$ & $1.6024\pm0.0175$ & $1.5934\pm0.0193$ \\ \hline
0.0100 & 1000 & 10 & $271\pm46$ & $1.5876\pm0.0182$ & $1.5863\pm0.0164$ \\ \hline

0.0200 & 100 & 4 & $460\pm106$ & $1.7245\pm0.0227$ & $1.6928\pm0.0200$ \\ \hline
0.0200 & 100 & 6 & $347\pm57$ & $1.7056\pm0.0141$ & $1.6774\pm0.0176$ \\ \hline
0.0200 & 100 & 8 & $421\pm78$ & $1.7215\pm0.0163$ & $1.6920\pm0.0149$ \\ \hline
0.0200 & 100 & 10 & $366\pm80$ & $1.7120\pm0.0208$ & $1.6888\pm0.0179$ \\ \hline
0.0200 & 200 & 4 & $265\pm67$ & $1.6716\pm0.0333$ & $1.6549\pm0.0265$ \\ \hline
0.0200 & 200 & 6 & $315\pm72$ & $1.6893\pm0.0192$ & $1.6688\pm0.0147$ \\ \hline
0.0200 & 200 & 8 & $283\pm57$ & $1.6870\pm0.0154$ & $1.6627\pm0.0156$ \\ \hline
0.0200 & 200 & 10 & $301\pm58$ & $1.6903\pm0.0268$ & $1.6662\pm0.0197$ \\ \hline
0.0200 & 500 & 4 & $351\pm42$ & $1.7028\pm0.0094$ & $1.6774\pm0.0098$ \\ \hline
0.0200 & 500 & 6 & $348\pm61$ & $1.6941\pm0.0202$ & $1.6688\pm0.0125$ \\ \hline
0.0200 & 500 & 8 & $298\pm69$ & $1.6793\pm0.0194$ & $1.6570\pm0.0216$ \\ \hline
0.0200 & 500 & 10 & $260\pm23$ & $1.6789\pm0.0076$ & $1.6551\pm0.0041$ \\ \hline
0.0200 & 1000 & 4 & $226\pm58$ & $1.6568\pm0.0287$ & $1.6468\pm0.0176$ \\ \hline
0.0200 & 1000 & 6 & $280\pm68$ & $1.6850\pm0.0162$ & $1.6672\pm0.0144$ \\ \hline
0.0200 & 1000 & 8 & $281\pm65$ & $1.6955\pm0.0212$ & $1.6700\pm0.0194$ \\ \hline
0.0200 & 1000 & 10 & $263\pm47$ & $1.6812\pm0.0218$ & $1.6632\pm0.0214$ \\ \hline

0.0500 & 100 & 4 & $3416\pm655$ & $1.8302\pm0.0144$ & $1.7872\pm0.0091$ \\ \hline
0.0500 & 100 & 6 & $2173\pm1092$ & $1.8095\pm0.0192$ & $1.7755\pm0.0121$ \\ \hline
0.0500 & 100 & 8 & $2263\pm1887$ & $1.8103\pm0.0308$ & $1.7727\pm0.0215$ \\ \hline
0.0500 & 100 & 10 & $2284\pm1657$ & $1.8143\pm0.0278$ & $1.7784\pm0.0166$ \\ \hline
0.0500 & 200 & 4 & $1283\pm324$ & $1.7947\pm0.0028$ & $1.7668\pm0.0036$ \\ \hline
0.0500 & 200 & 6 & $760\pm601$ & $1.7773\pm0.0202$ & $1.7478\pm0.0197$ \\ \hline
0.0500 & 200 & 8 & $705\pm712$ & $1.7730\pm0.0176$ & $1.7473\pm0.0166$ \\ \hline
0.0500 & 200 & 10 & $520\pm273$ & $1.7741\pm0.0155$ & $1.7455\pm0.0154$ \\ \hline
0.0500 & 500 & 4 & $532\pm321$ & $1.7731\pm0.0167$ & $1.7431\pm0.0167$ \\ \hline
0.0500 & 500 & 6 & $382\pm95$ & $1.7654\pm0.0114$ & $1.7386\pm0.0138$ \\ \hline
0.0500 & 500 & 8 & $291\pm74$ & $1.7577\pm0.0101$ & $1.7292\pm0.0136$ \\ \hline
0.0500 & 500 & 10 & $291\pm54$ & $1.7572\pm0.0095$ & $1.7284\pm0.0105$ \\ \hline
0.0500 & 1000 & 4 & $277\pm45$ & $1.7542\pm0.0071$ & $1.7284\pm0.0094$ \\ \hline
0.0500 & 1000 & 6 & $248\pm52$ & $1.7497\pm0.0065$ & $1.7191\pm0.0135$ \\ \hline
0.0500 & 1000 & 8 & $244\pm50$ & $1.7464\pm0.0132$ & $1.7197\pm0.0142$ \\ \hline
0.0500 & 1000 & 10 & $250\pm48$ & $1.7460\pm0.0194$ & $1.7220\pm0.0166$ \\ \hline

0.1000 & 100 & 4 & $4962\pm82$ & $1.8549\pm0.0102$ & $1.7948\pm0.0065$ \\ \hline
0.1000 & 100 & 6 & $4951\pm63$ & $1.8767\pm0.0093$ & $1.8018\pm0.0059$ \\ \hline
\textbf{0.1000} & \textbf{100} & \textbf{8} & \textbf{$4892\pm99$} & \textbf{$1.8852\pm0.0088$} & \textbf{$1.8070\pm0.0048$} \\ \hline
0.1000 & 100 & 10 & $4972\pm27$ & $1.8841\pm0.0120$ & $1.8042\pm0.0054$ \\ \hline

% missing tests with epoch size = 200

0.1000 & 500 & 4 & $2866\pm1435$ & $1.8466\pm0.0285$ & $1.7884\pm0.0143$ \\ \hline
0.1000 & 500 & 6 & $2089\pm421$ & $1.8416\pm0.0133$ & $1.7908\pm0.0057$ \\ \hline
0.1000 & 500 & 8 & $1286\pm868$ & $1.8160\pm0.0322$ & $1.7770\pm0.0156$ \\ \hline
0.1000 & 500 & 10 & $1296\pm652$ & $1.8156\pm0.0316$ & $1.7705\pm0.0204$ \\ \hline
0.1000 & 1000 & 4 & $738\pm607$ & $1.7988\pm0.0283$ & $1.7624\pm0.0205$ \\ \hline
0.1000 & 1000 & 6 & $977\pm408$ & $1.8098\pm0.0139$ & $1.7757\pm0.0080$ \\ \hline
0.1000 & 1000 & 8 & $815\pm285$ & $1.8010\pm0.0078$ & $1.7706\pm0.0034$ \\ \hline
0.1000 & 1000 & 10 & $272\pm116$ & $1.7778\pm0.0160$ & $1.7498\pm0.0148$ \\ \hline

\end{tabular}}
\renewcommand{\baselinestretch}{1.0}
\end{center}
\caption{Análise dos parâmetros de treinamento para um detetor neural cuja a
entrada são os 100 anéis resultantes do processamento de uma RoI.}
\label{tab:ringer-param-optimization}
\end{table}

Os testes realizados sugerem que uma combinação entre a taxa de treinamento
igual à $0,01$, o tamanho da época de treinamento igual à $100$ e o número de
neurônios escondidos igual à $8$ maximizem o potencial discriminante do
sistema, que já se apresenta superior aos testes usando as características do
T2Calo. Uma análise da tabela indica que a utilização de taxas de treinamento
muito pequenas, para este sistema, é prejudicial. Isto pode ocorrer seja
quando a superfície de erro não possui uma inclinação relativamente acentuada
que faça o sistema convergir de forma suficiente rápida, seja pela existência
de mínimos locais, ou ainda, uma combinação destas duas
características. Realizou-se, igualmente, testes com taxas de aprendizado
maior que $0,01$. Estes testes demonstraram grande instabilidade na fase de
treinamento e foram descartados como possibilidade para o sistema final.

O número de neurônios escondidos ideal aparenta estar entre 8 e 10. Os testes
indicam que exista pouca diferença estatítisca entre as duas opções e portanto
escolhe-se 8. Esta escolha se traduz em um sistema de deteção menos complexo,
o que é positivo em geral. O tamanho ideal da época de treinamento, ou
batelada parece ter uma correlação bastante forte com a qualidade do
discriminador final. Quanto maior a época, menor a eficiência do detetor, em
média, tendo-se por vista os valores de produto SP destacados. Mais uma vez,
realizou-se testes com valores para o tamanho da época menores que 100. Estes
testes demonstraram maior instabilidade no treinamento sem que houvesse um
aumento no valor médio do produto SP para o conjunto de testes. Desta forma,
concluiu-se que os valores escolhidos da
Tabela~\ref{tab:ringer-param-optimization} representem um bom compromisso 
dentre os parâmetros de treinamento que se deseja otimizar para este detetor.

\subsubsection{Resultados para o \eng{NeuralRinger}}

Baseando-se nos parâmetros destacados acima, treinou-se dez redes que tiveram
seus pesos sinápticos iniciais fixados em valores aleatórios e diferentes para
cada uma destas iterações. A Figura~\ref{fig:ringer-mlp-mse} mostra a evolução
do EMQ de uma destas redes. A Figura~\ref{fig:ringer-mlp-sp} mostra um gráfico
equivalente para a evolução do produto SP ao longo do treinamento da
rede. Como é possível distinguir, após cerca de 3000 passos de treinamento,
apesar do EMQ continuar a apresentar declínio, a capacidade discriminante da
rede parece pouco influenciada. A capacidade discriminante no que tange o
conjunto de treinamento, apesar da constatação de estagnação do conjunto de
teste, ainda continua a aumentar após os 3000 passos. É interessante observar,
no entanto, que a capacidade discriminante da rede, para o conjunto de teste,
resta inalterada não indicando a ocorrência do fenômeno de
\eng{overtraining}. O mesmo comportamento pode ser observado com a evolução do
EMQ.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/mse-evolution}
\end{center}
\caption{Evolução do EMQ para os conjuntos de treino e teste de um
discriminador elétron/jato neural para as variáveis do anelador.}
\label{fig:ringer-mlp-mse}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/sp-evolution}
\end{center}
\caption{Evolução do produto SP para os conjuntos de treino e teste de um
discriminador elétron/jato neural para as variáveis do anelador.}
\label{fig:ringer-mlp-sp}
\end{figure}

A Figura~\ref{fig:ringer-mlp-output} contém a saída da rede após o treinamento
do detetor neural. Diferentemente aos casos anteriores, o sistema parece
identificar com bastante precisão elétrons e jatos. Um corte em $+0,0302$
estabele o máximo do produto SP (conjunto de teste) para esta rede, com uma
eficiência de $96,55$\% na deteção de elétrons contra apenas $3,12$\% de
falso-alarme em jatos. O produto SP para estes valores é $\approx 1,81$,
representando o melhor valor para este índice obtido até o momento. Este valor
está $0,3$ pontos acima do valor obtido para as saídas do T2Calo usando um
detetor neural ou baseado no algoritmo LMS!

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/test-output}
\end{center}
\caption{Saída para o conjunto de teste, do discriminador neural baseado nas
características extraídas pelo anelador.}
\label{fig:ringer-mlp-output}
\end{figure}

A Figura~\ref{fig:ringer-test-roc} contém um comparativo das R.O.C. entre
todos métodos estudados até o momento. Como é possível perceber, o método
proposto de anelamento, associado a um detetor neural produz um resultado
significativamente melhor para a massa de dados avaliada. Para um mesmo valor
de eficiência na deteção de elétrons, por exemplo $90$\%, o sistema baseado no
anelador aprovará apenas cerca de 400~Hz em jatos ao passo que o sistema
baseado na solução atual (EGammaHypo) para o LVL2, cerca de 2,4~kHz. Por outro
ângulo este resultado traz que, para uma mesma capacidade de aprovar elétrons,
um sistema baseado no anelamento e redes neurais poupará 6 vezes mais o
terceiro nível de filtragem do experimento ATLAS. Se, por outro lado deseja-se
aumentar a eficiência na deteção de elétrons, o anelador consegue, até cerca
de $97$\% de eficiência, manter uma taxa de falso-alarme igual ou inferior a
1~kHz. Para o sistema atual baseado no EGammaHypo, para uma eficiência na
deteção de elétrons de $97$\%, $100$\% dos jatos deveria ser aprovados, o que
seria inviável.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/mlp-vs-lms-vs-egamma-roc}
\end{center}
\caption{R.O.C. para o conjunto de teste, do discriminador neural baseado nas
características extraídas pelo anelador, comparado com os resultados obtidos
para o T2Calo.}
\label{fig:ringer-test-roc}
\end{figure}

As Figuras~\ref{fig:ringer-test-sp-eta} e \ref{fig:ringer-test-sp-phi} mostram
os valores do produto SP por intervalo em $\eta$ e em $\phi$,
respectivamente. Observa-se, tal qual como nos sistemas anteriores, que a
eficiência de deteção é menor próxima à região do \eng{crack}, no entanto,
maior que nos outros casos. Na parte central do detetor, onde maior parte dos
eventos de interesse se concentrarão, o valor do produto SP é bastante alto,
chegando, em alguns pontos, a aproximar-se de $2,0$. O gráfico equivalente em
$\phi$ mostra a homogenidade esperada.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/test-sp-eta}
\end{center}
\caption{Análise do produto SP ao longo de $\eta$ para o discriminador neural
baseado nas saídas do anelador.}
\label{fig:ringer-test-sp-eta}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/test-sp-phi}
\end{center}
\caption{Análise do produto SP ao longo de $\phi$ para o discriminador neural
baseado nas saídas do anelador.}
\label{fig:ringer-test-sp-phi}
\end{figure}

A Figura~\ref{fig:ringer-test-sp-emet} contém uma análise do produto SP por
intervalo em $\etem$. A eficiência de deteção neste caso apresenta-se
constante, ou aproximadamente constante, até a faixa de energia em torno de
100~GeV, caido abruptamente. Os número no topo de cada barra indicam a
quantidade de RoI's analisadas no intervalo e servem como uma medida da
significância do resultado. Para valores de energia acima de $\approx
100$~GeV, o número de eventos disponíveis para treino e análise cai
significativamente, devido à natureza dos eventos estudados. Por esta razão,
acredita-se que os resultados para altos valores energéticos devam ser
interpretados com maior cuidado.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/test-sp-emet}
\end{center}
\caption{Análise do produto SP ao longo de $E^{e.m.}_T$ para o discriminador
neural baseado nas saídas do anelador.}
\label{fig:ringer-test-sp-emet}
\end{figure}

As Figuras~\ref{fig:ringer-mse-relevance} e \ref{fig:ringer-sp-relevance}
trazem as análises de relevância para o conjunto de teste, das 100 variáveis
do anelador, tendo em vista a variação do EMQ e do produto SP
respectivamente. Os dois gráficos parecem indicar, igualmente, uma grande
importância nos primeiros anéis de cada camada (veja
Tabela~\ref{tab:ringer-position}). A importância de cada anel diminui com o
afastamento do centro, onde pressupõe-se que a relação sinal ruído seja
inferior. A importância relativa entre os anéis mais importantes e aqueles de
menor importância parece ser menor da perspectiva da relevância de baseada no
produto SP que da relevância clássica. Outra informação que pode ser aferida é
sobre a importância da informação na primeira camada e.m.. Para a relevância
clássica, as variáveis desta camada constituem uma informação essencial na
determinação da saída, estando ao menos uma ordem de magnitude acima da
relevância dos dados de outras camadas. Na avaliação baseada na capacidade
discriminante do sistema, a parte hadrônica é a mais importante. A importância
relativa entre os dados da primeira e segunda camadas e.m. e da primeira
camada hadrônica também difere. Na análise da relevância utilizando a
discriminação, estas três camadas parecem exercer um papel igualmente
importante na deteção de elétrons e jatos.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/test-mse-relevance}
\end{center}
\caption{Os valores de relevância para os conjunto de teste, para as 100
variáveis do anelador e considerando-se o classificador neural em estudo.}
\label{fig:ringer-mse-relevance}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/test-sp-relevance}
\end{center}
\caption{Os valores de relevância de discriminação para os conjunto de teste,
para as 100 variáveis do anelador e considerando-se o classificador neural em
estudo.}
\label{fig:ringer-sp-relevance}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|} \hline
\textbf{Camada} & \textbf{Número de anéis} & \textbf{Primeiro} &
\textbf{Último} \\ \hline
Pré-irradiador & 8 & 1 & 8 \\
1\eira e.m. & 64 & 9 & 72 \\
2\eira e.m. & 8 & 73 & 80 \\
3\eira e.m. & 8 & 81 & 88 \\
1\eira hadrônica & 4 & 89 & 92 \\
2\eira hadrônica & 4 & 93 & 96 \\
3\eira hadrônica & 4 & 97 & 100 \\ \hline
\end{tabular}
\end{center}
\caption{Posicionamento relativo dos anéis produzidos pelo anelador no vetor
de entrada ao sistema de deteção neural.}
\label{tab:ringer-position}
\end{table}

\subsection{Compressão baseada na relevância}

A informação de relevância estudada até aqui pode ser aproveitada para que
aplique-se sobre as entradas do discriminador neural, uma poda baseada na
capacidade discriminante de uma variável. Esta poda pode ser feita
automaticamente \cite{andre-enfpc2000} em muitos casos, ou manualmente se
existirem restrições no processo de poda.

De fato, a técnica de normalização seqüêncial dos anéis que é proposta nesta
seção assume a disponbilidade da energia total camada a camada para o cálculo
do fator de normalização para cada anel. Ademais, para calcular o fator de
normalização para o anel $N+1$ em uma mesma camada, necessita-se do valor de
energia do anel $N$. Portanto, se deseja-se manter o anel $N+1$ de uma camada,
é necessário que se mantenha o anel $N$, ao menos para o cálculo do fator de
normalização. De fato, como veremos mais a frente, esta última sugestão seria
uma prática que recompensaria em pouco, visto que a maior parte do tempo de
processamento não está na discriminação neural e sim na aquisição dos
dados. Por outro lado, seria possível considerar que uma poda sempre seria
positiva no que tange a complexidade do discriminador neural que segue o
sistema de anelamento. Ao retirar uma variável da entrada, ao menos 8 conexões
sinápticas do detetor final seriam removidas simultaneamente, levando-se em
consideração a configuração exemplificada anteriormente. 

Observando-se mais uma vez o gráfico da Figura~\ref{fig:ringer-sp-relevance},
podemos concluir que o grau de importância dos anéis diminui, num geral, do
pico de deposição energética ($N = 0$) para as bordas da RoI. Desta forma o
problema da otimização deste sistema, numa primeira aproximação, poderia se
resumir à poda de elementos da periferia, de acordo com a relevância de
discriminação observada naquele gráfico.

Outra técnica que empregaremos será a combinação de camadas menos relevantes,
reduzindo o número de entradas do processador neural, sem que se perca,
necessariamente, a informação de energia presente nos anéis removidos. Uma
outra forma de manter a informação de energia, mas compactando ainda mais a
entrada seria a utilização de anéis cuja a largura dependesse da relevância
dos dados naquela região. Este estudo, reservaremos a um outro projeto. Tendo
em vista a discussão nos parâgrafos anteriores, propõe-se a seguinte variante
do processo de anelamento, baseando-se na relevância de discriminação da
Figura~\ref{fig:ringer-sp-relevance}

\begin{enumerate}
\item \textbf{Pré-irradiador}: utilizar apenas 4 ao invés de 8 anéis, com a
mesma granularidade anterior;
\item \textbf{1\eira\ camada e.m.}: utilizar apenas 12 dos 64 anéis iniciais, com a
mesma granularidade anterior;
\item \textbf{2\eira\ camada e.m.}: utilizar apenas 6 dos 8 anéis iniciais, com a
mesma granularidade anterior;
\item \textbf{3\eira\ camada e.m.}: utilizar apenas 4 dos 8 anéis iniciais, com a
mesma granularidade anterior;
\item \textbf{1\eira\ camada hadrônica}: manter a configuração atual;
\item \textbf{2\eira\ e 3\eira\ camadas hadrônica}: aglutinar em uma mesma
``camada'' virtual, mantendo a granularidade anterior.
\end{enumerate} 

Um novo total de 34 anéis é calculado da massa de dados originais. Em seguida,
cinco testes mantendo a mesma parametrização do sistema anterior foram
conduzidos. A R.O.C. do melhor destes testes, junto àquelas dos testes
anteriores, pode ser vista na Figura~\ref{fig:relev-cut1-roc}. A figura mostra
que o sistema podado se comporta tão bem quanto o sistema original com o 100
anéis, o superando em alguns pontos. Este novo sistema possui apenas a
terceira parte do número de anéis do sistema anterior, mas tem um desempenho
marginalmente melhor. O discriminador neural tem $66 \times 8 = 528$ menos
conexões sinápticas, somando um total de $289$ sinapses. O máximo do produto
SP ($1,82$) é atingido para uma eficiência na deteção de elétrons de $96,6$\%
contra um falso-alarme em jatos de apenas $2,94$\% (735~Hz).

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/relevance-cut1-roc}
\end{center}
\caption{R.O.C. comparativa entre os sistemas de deteção de elétrons e jatos
analisados até agora e o novo sistema de deteção gerado à partir da poda dos
canais de entrada baseada na relevância.}
\label{fig:relev-cut1-roc}
\end{figure}

A Figura~\ref{fig:relevance-cut1-sp-relevance} contém os valores de relevância
de discriminação para o detetor em questão. A
Tabela~\ref{tab:ringer-position-relevance-cut1} contém a posição relativa de
cada anel dentro do conjunto. A relação de relevância entre os anéis mudou
para este sistema. Anteriormente, os anéis da parte hadrônica eram os mais
relevantes ao processo discriminativo, a passo que neste novo sistema, os
anéis da primeira camada e.m. se mostram mais importantes. Dentro desta
camada, a ordem de importância não decresce com a distância ao pico de
deposição energética, apresentando um padrão menos estruturado.

A primeira camada e.m., como mostra a Figura~\ref{fig:lar-detail} no
Capítulo~\ref{chap:atlas}, é uma camada mais fina, originalmente concebida
para a deteção de jatos de partículas \cite{lar-tdr}. Estes objetos podem ser
também caracterizados analisando-se os padrões de deposição nesta camada:
múltiplas células sensibilizadas dentro de uma mesma RoI aumentam a
probabilidade do objeto em análise não ser um elétron. De fato, a variável
$\eratio$ do T2Calo assume exatamente este papel, como explicado no início do
Capítulo~\ref{chap:baseline}. Por esta razão, é bastante intuitivo que os
dados na primeira camada e.m. apresentem um padrão de relevância distinto aos
das demais camadas. De fato, o mesmo fenômeno pode também ser observado na
Figura`\ref{fig:ringer-sp-relevance}, embora neste contexto a informação
esteja menos evidente dado o contraste entre valores de relevância baixos para
os outros aneís. Para as outras camadas, a relevância parece ser maior para o
segundo anel que para o primeiro, caindo à medida que se afasta do centro.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/relevance-cut1-sp-relevance-test}
\end{center}
\caption{Relevâncias de discriminação para o detetor neural baseado em 34 (dos
100) anéis resultantes da poda por relevância.}
\label{fig:relevance-cut1-sp-relevance}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|} \hline
\textbf{Camada} & \textbf{Número de anéis} & \textbf{Primeiro} &
\textbf{Último} \\ \hline
Pré-irradiador & 4 & 1 & 4 \\
1\eira e.m. & 12 & 5 & 16 \\
2\eira e.m. & 6 & 17 & 22 \\
3\eira e.m. & 4 & 23 & 26 \\
1\eira hadrônica & 4 & 27 & 30 \\
2\eira e 3\eira\ hadrônicas & 4 & 31 & 34 \\ \hline
\end{tabular}
\end{center}
\caption{Posicionamento relativo dos anéis produzidos pelo sistema de
anelamento podado para ter apenas 34 (dos 100) anéis.}
\label{tab:ringer-position-relevance-cut1}
\end{table}

%Levando-se em consideração a largura dos anéis e uma RoI que tem por base um
%tamanho de $\Delta_\eta\times\Delta_\phi = 0,4 \times 0,4$. Este novo sistema
%utiliza uma área cerca 

Uma vez que a eficiência deste novo sistema de deteção não foi afetada pela
poda, propõe-se uma segunda poda com a seguinte configuração:

\begin{enumerate}
\item \textbf{Pré-irradiador}: não se utilizará os dados desta camada;
\item \textbf{1\eira\ camada e.m.}: utilizar apenas 6 dos 64 anéis iniciais,
com a mesma granularidade anterior;
\item \textbf{2\eira\ camada e.m.}: utilizar apenas 4 dos 8 anéis iniciais,
com a mesma granularidade anterior;
\item \textbf{3\eira\ camada e.m.}: não se utilizará os dados desta camada;
\item \textbf{1\eira\ camada hadrônica, 2\eira\ e 3\eira\ camadas hadrônica}: aglutinar em uma mesma ``camada'' virtual, mantendo a granularidade anterior.
\end{enumerate} 

Um total de 14 anéis restam nesta nova configuração. Mantém-se os parâmetros
de treinamento e o número de neurônios na camada escondida do detetor neural
inicial. A Figura~\ref{fig:relev-cut2-roc} mostra a curva R.O.C. de um dos 5
testes realizados, junto à curva R.O.C. dos outros detetores. Mais uma vez o
sistema não apresenta perda no desempenho comparado ao sistema original com
100 anéis. O ponto de máximo do produto SP ocorre quando a eficiência na
deteção de elétrons é de $96,89$\% contra um falso-alarme de $2,72$\%
(680~Hz).

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/relevance-cut2-roc}
\end{center}
\caption{R.O.C. comparativa entre os sistemas de deteção de elétrons e jatos
analisados até agora e o novo sistema de deteção baseado na poda da
relevância, com apenas 14 (dos 100) anéis.}
\label{fig:relev-cut2-roc}
\end{figure}

A Figura~\ref{fig:relevance-cut2-sp-relevance} contém os valores de relevância
de discriminação para a segunda poda baseada no critério da relevância de
discriminação. A Tabela~\ref{tab:ringer-position-relevance-cut2} mostra a
relação entre os números naquele gráfico. Neste novo sistema, a parte
hadrônica re-adquire a importância do sistema baseado nos 100 anéis,
recuperando, possivelmente, a falta de dados para a terceira camada e.m., que
foi suprimida nesta avaliação.

\begin{figure}
\begin{center}
\includegraphics[scale=0.98]{ringer-mlp/relevance-cut2-sp-relevance-test}
\end{center}
\caption{Relevâncias de discriminação para o detetor neural baseado em 14 (dos
100) anéis resultantes da poda por relevância.}
\label{fig:relevance-cut2-sp-relevance}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|} \hline
\textbf{Camada} & \textbf{Número de anéis} & \textbf{Primeiro} &
\textbf{Último} \\ \hline
1\eira e.m. & 6 & 1 & 6 \\
2\eira e.m. & 4 & 7 & 10 \\
1\eira\, 2\eira e 3\eira\ hadrônicas & 4 & 11 & 14 \\ \hline
\end{tabular}
\end{center}
\caption{Posicionamento relativo dos anéis produzidos pelo sistema de
anelamento podado usando a análise de relevância discriminante, para ter
apenas 14 (dos 100) anéis.}
\label{tab:ringer-position-relevance-cut2}
\end{table}

\typeout{ *************** End of file neural.tex *************** }
